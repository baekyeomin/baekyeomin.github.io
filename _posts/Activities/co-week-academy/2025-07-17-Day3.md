---
title: "[Co-Week Academy] Day3"
categories: [activities, co-week-academy]
tags:
  - AI
layout: single
toc : true
toc_sticky: true
comments: true
---

제4회 CO-WEEK Academy 3일차 강의 내용을 정리한 학습 기록입니다.
{: .notice--info}


## 0. 강의 전 쫑알 쫑알
**_"네가 너인 게 어떻게 네 약점이 될 수 있어?"_**
지난 밤에 **<대도시의 사랑법>**이라는 영화를 보고 잤어요. <br>
거기 나오는 남자 주인공, 흥수가 작가를 꿈꾸며 글을 쓰는 장면에서 첫 문장으로 등장하는 문장입니다.  <br> 
자세한 내용은 스포일러가 될까봐 적지 않을게요 !
정말 명대사가 많은, 잔잔한 여운을 남기는 영화니, 위로 받고 싶은 날에 추천하는 영화입니당 😉

---

## 1. 자율주행을 위한 AI 기술
### 🐾 강의자 정보
- 교수님 : 조영은 교수님
- 소속 : 한양대 ERICA
- 이메일 : choye@hanyang.ac.kr

### 🐾 적용가능한 AI 기술 분야
자율 주행에서 인공지능 기술은 다음과 깉은 분야에 활용됩니다.  

- **이미지 분류 (Image Classification)**  
  : 이미지에 어떤 물체가 있는지를 분류 (예: 차량, 사람, 신호등 등)

- **의미론적 분할 (Semantic Segmentation)**  
  : 이미지 속 각 픽셀을 의미 있는 카테고리(예: 도로, 인도, 차량 등)로 분류

- **객체 탐지 (Object Detection)**  
  : 이미지에서 객체의 위치와 종류를 함께 탐지 (Bounding Box + 클래스 분류)

- **인스턴스 분할 (Instance Segmentation)**  
  : 같은 클래스 내에서도 개별 객체를 구분하여 더욱 정밀한 인식 가능

→ 이 중 **Object Detection**이 자율주행 환경 인식의 핵심 기술로 사용


### 🐾 주요 Object Detection의 어려움
- **Intra-class variation**: 같은 클래스라도 모양/색상이 다양함  
- **Viewpoint variation**: 관찰 각도에 따라 형태가 달라짐  
- **Illumination changes**: 낮/밤, 밝기 변화에 민감  
- **Background clutter**: 복잡한 배경 (예: 눈, 나무, 간판 등)  
- **Deformation**: 객체(특히 사람)의 움직임, 자세 변화  
- **Occlusion**: 사물 일부가 가려지는 경우  
- **Context matters**: 주변 맥락에 따라 해석 달라짐 (ex. 똑같이 생긴 기둥 → 사람? 소화전?)


### 🐾 성능 평가 지표
- **IoU (Intersection over Union)**   
  : 예측한 Bounding Box와 실제 정답 Box의 겹친 정도  
  - IoU > 0.5면  True Positive (TP)로 간주
  - IoU < 0.5면 False Negative(FN)으로 간주
  - 잘못된 위치나 중복 탐지 등 오류 탐지 수는 False Positive (FP)

- **Precision / Recall / Average Precision (AP)**  
  - **Precision**: 모델이 양성으로 예측한 것 중에서 실제로 양성인 비율
  - **Recall**: 실제 양성 중에서 모델이 정확하게 감지한 비율  
  - **AP**: 다양한 임계값(threshold)에 대해 precision-recall 곡선을 계산한 평균

- **mAP (mean Average Precision)**  
  : 여러 클래스에 대해 AP 값을 평균낸 지표 → 객체 탐지 모델 전체 성능을 종합적으로 평가하는데 쓰임


### 🐾 Object Detection 기술 흐름
#### 전통적인 방식 : Sliding Window
- 이미지를 고정된 크기의 윈도우로 잘라가며 전체 이미지 스캔  
- 각 영역마다 특징 추출 후 분류기(SVM, 랜덤포레스트 등)로 객체 판단  
- 다양한 스케일과 종횡비에 대응하기 위한 다중 해상도 처리  
- **단점**: 느리고 비효율적, 사람이 특징을 설계해야 함

#### 현대적인 방식 : CNN 기반
- **R-CNN**: 후보 영역마다 CNN 적용 → 느림  
- **Fast R-CNN**: 공유된 feature map에서 ROI Pooling으로 속도 개선  
- **Faster R-CNN**: Region Proposal도 CNN이 수행 (RPN 도입)  
- **Mask R-CNN**: 객체 분할까지 확장 가능


### 🐾 3D Object Detection
- 2D Box: (x, y, w, h)  
- 3D Box: (x, y, z, w, h, l, r, p, y) ← 회전 정보 포함  
- **LiDAR 기반** 거리 측정 가능 → 깊이 정보 보완  
- **Pseudo-LiDAR**: 카메라 이미지를 LiDAR처럼 가공해 3D 인식  
- **VoxelNet / PointNet++ / Mono R-CNN** 등 다양한 방식 존재  
- **BEV(Top View) 기반** 방식도 있음 → Pixor, HDNet 등



---
## 2. 미래자동차의 점진적 지능화를 위한 강화학습에 대한 이해
(DAY2 강화학습 강의와 겹치는 내용이 너무 많아서 생략)


---
## 3. 다중모달 데이터를 활용한 딥러닝
### 🐾 강의자 정보
- 교수님 : 조대웅 교수님
- 소속 : 경북대학교
- 이메일 : daeung.jo@knu.ac.kr

### 🐾 다중모달 AI 정의
- 서로 다른 형태의 데이터들을 통합적으로 활용하는 인공지능

### 🐾 다중모달 AI 필요 이유
1. 현실 세계에 대한 더욱 깊은 이해
- 현실에서 일어나는 일들은 대부분 단일 정보만으로는 파악이 어렵고, 시각, 청각 등 여러 감각이 동시에 작용하는 복합적인 정보들로 이루어져 있다.
- ex. 비디오 = 이미지 + 음성 + 시간 ...
- ex. 자율주행 = RGB 카메라 + 라이다 + 래이다… 등의 센서
- ex. 의료 데이터 = 영상을 활용한 진단 생성 (X ray, text 라는 두가지 모달리티 사용)
- 삶 역시 다양한 감각기관을 통해 정보를 수집하고 이를 종합적으로 해석해 세상을 이해하듯, 인공지능도 멀티모달 데이터를 통해 더 정확하게 세상을 인식할 수 있다. 
- ex . **맥거크 효과(McGurk Effect)** 
    - 같은 소리라도 입모양에 따라 다르게 인식하는 효과
    - 청각정보와 시각 정보가 충돌하는 경우에 단일 감각만으로는 완전한 인지가 어렵다는 점을 보여줌
- ex. **Bouba/Kiki 효과**
    - 이미지와 언어 사이 인간은 직관적인 연관성을 느낀다는 것을 보여주는 실험

2. 멀티모달 데이터를 통해 사람과의 상호작용성 증대
- 사람 간의 소통은 글, 말, 제스처 등 다양한 수단을 통해 이루어지는데, 인공지능이 사람을 잘 도와주기 위해서는 AI 역시 멀티모달 데이터를 활용할 줄 알아야한다. 


### 🐾 다중모달 AI에서의 중요 문제들
#### 모달리티 간 차이 (모달리티 갭)
- 각 센서나 데이터는 서로 다른 특징을 가지고 있는데, 이런 격차를 이해하고 조화롭게 활용하는 것이 멀티모달 AI의 핵심 과제
- 멀티 모달 데이터의 구성요소
    - Modality-General 정보: 모든 모달리티에서 공유되는 공통 정보
    - Modality-Specific 정보: 각 모달리티만이 제공할 수 있는 고유 정보

#### 데이터 확보의 어려움
- 멀티모달 데이터를 수집할 때, 센서 고장, 배터리 문제, 저장 공간 부족 등으로 **데이터 누락(missing data)** 발생 가능
- 센서 수가 많아질수록 전체 데이터의 완전성은 떨어질 수 있다. 
- 동기화 문제도 有
    - 멀티모달 데이터는 시공간적으로 정렬되어야 의미 O

#### 실시간 처리의 한계
- 멀티모달 모델은 여러 센서 데이터를 동시에 처리하기에 연산량이 많고, 실시간 구동에 어려움 O
- 하드웨어 제약 O
    - 연산 속도, 전력 소모, 발열 문제 등 다양한 물리적 한계가 존재할 수 있다. 
- 비용 문제 O


### 🐾 다중모달 AI 관련 연구
1. **Multi-modal Fusion**
- 다양한 방식으로 모달리티를 융합하는 방법
- data level fusion :  raw data들을 바로 융합하는 방식
- feature level fusion : 추출된 feature 단계에서 융합하는 방식 
    - early fusion : 직접적인 모달리티 간 상호작용 학습 가능 / 높은 정합 난이도, 입력 단계에서 모든 모달 요구 / 결측 모달 대응 및 설계 유연성 부족 / 높은 연산 및 메모리 요구
- output level fusion : 각 모달리티 모델 별 예측 결과를 유합하는 방
    - late fusion : 낮은 정합 난이도 / 모달리티 별로 모델을 독립적으로 운용가능
- hybrid fusion: 융합 방식을 복합적으로 활용
- 이외에도 fusion by concatenation / fusion by transformer (only text, text + image) 등이 있음

2. **Feature Alignment**
- 서로 다른 모달리티 → 공통 잠재 공간(Latent Space)으로 투영
- 비교 및 결합 용이
- 예시: CLIP, 의료 AI 등

3. **Cross-modal Knowledge Distillation**
- 교차 모달 지식 증류: 고성능 모달리티로 저성능 모달리티 지도
- 예: 라이다 모델이 카메라 기반 모델 학습을 도와 성능 향상
- 테스트 시에는 경량 모델만 사용 → 실용성 ↑

4. **Robust Model Learning**
- 결측/노이즈 있는 모달리티 대응


<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}