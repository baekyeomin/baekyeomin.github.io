---
title: "[Co-Week Academy] Day2"
categories: [activities, co-week-academy]
tags:
  - AI
layout: single
toc : true
toc_sticky: true
comments: true
---

제4회 CO-WEEK Academy 2일차 강의 내용을 정리한 학습 기록입니다.
{: .notice--info}


## 0. 강의 전 쫑알 쫑알
오늘은 코위크하면서 맞는 첫 아침이었어용
저는 전날 밤에 매일매일 조식을 챙겨 먹겠다했지만,,
막상 아침이 되니까... 저는 조식보다 잠을 더 사랑하는 사람인가봐요.. ㅎ..ㅎ
그래서 오늘이 처음이자 마지막 조식이 되었습니다 💤
어쨌든 비몽사몽 걸어가 먹은 조식,
**생각보다 맛있자나? **
후식으로 나온 플레인 요플레도 야무지게 챙겨서,나중에 야식으로 먹으려고 냉동실에 슬쩍 넣어뒀습니댜 히히
![코위크 식단표](/assets/images/coweek_photo3.jpg)


---

## 1. 코드 거대 언어 모델
### 🐾 강의자 정보
- 교수님 : 최윤석 교수님
- 소속 : 성균관대학교
- 이메일 : ys.choi@skku.edu

### 🐾 코드와 자연어 비교
| 구분 | 자연어 (NL) | 프로그래밍 언어 (PL) |
|------|-------------|----------------------|
| 형태 | 유연함 | 엄격한 문법 |
| 예시 | 영어, 한국어 등 | Python, Java 등 |
| 구성 | 단어, 문장 | 변수, 함수, 제어문 등 |

- 그치만 코드도 sequence(연속적인 데이터)로 보고 자연어처럼 처리 가능


### 🐾 코드 인공지능의 주요 작업
#### 1. 이해하기
- **Code Search**: 자연어로 작성된 질문과 가장 관련 있는 코드 찾기
- **Code Classification**: 언어 분류, 버그 유무 판단 등
- **Code Clone Detection**: 코드 유사성 탐지

-> 대부분은 _분류_ 문제이다.

#### 2. 생성하기
- **Code Summarization**: 코드가 주어지면 해당 코드에 대한 내용을 자연어 형태로 요약해주는 것
- **Code Generation**: 요구사항 기반 코드 자동 생성
- **Code Translation**: 코드를 다른 언어로 바꾸기
등


### 🐾 대표적인 pre-trained 코드 언어 모델 (LM)
- 대부분 트랜스포머 기반
- comments와 code로 pre-trained 됨


| 모델 | 특징 | 논문/연도 |
|------|------|-----------|
| **CodeBERT** | NL + PL 양방향 학습 | EMNLP 2020 |
| **GraphCodeBERT** | AST(구문 트리) 구조 학습 | ICLR 2021 |
| **PLBART** | 생성 + 이해 모두 가능 | NAACL 2021 |
| **CodeT5** | identifier-aware, encoder-decode, identifier에만 mask 씌워서 코드 좀 더 잘 understand하게.. | EMNLP 2021 |
| **UniXcoder** | 인코더 디코더를 하나로 통일하고 prefix 활용한 unified 모델 | ACL 2022 |
| **CodeT5+** | 대형 모델 확장 (16B까지) | EMNLP 2023 |
| **Code Llama** | Llama 기반 코드 전용 모델 | arXiv 2023 |
| **CodeGemma** | Gemini 기반, 빠른 생성 | arXiv 2024 |
| **DeepSeek-Coder-v2** | 최대 128k 컨텍스트, 236B 파라미터 | arXiv 2024 |


### 🐾 코드 표현 방법
- **Sequence-based**: 코드 = 토큰 시퀀스  
- **Graph-based**: AST, Control/Data Flow Graph


### 🐾 주요 도전 과제 (Challenges)
#### 1. Semantic Gap between NL & PL
- 자연어 ↔ 코드 서로 다른 모달리티인데 갭을 줄일 수 있는가?

#### 2. Robustness
- Dead code나 순서 바뀐 코드를 잘 이해하는가?
    - 데드코드 추가한 것이 아닌 것과 같냐고 물어보면?
    -> 출력은 같은데 다르다고 판단함
    -> LM은 코드를 이해하는게 아니라 코드에 있는 자연어를 기반으로 이해하나..?

#### 3. Hallucination
- 존재하지 않는 함수/코드 생성
- **해결 시도**: RAG, API 문서 검색, 컨텍스트 확장

#### 4. Evaluation
- 정답과의 **문법적 일치**뿐만 아니라 **실행 가능성 평가** 필요
- `pass@k`, CodeBERTScore 등..

#### 5. 테스트 생성
- 자동 생성된 테스트가 실행 가능한가?
- SBST + LLM 협업

#### 6. 멀티모달 (VLM)
- Vision-Language 모델로 **단계적 reasoning 필요**
- VisProg, ViperGPT 등..


### 🐾 짧은 소감 ?
코드와 자연어의 구조적 차이부터 시작해서, CodeBERT 같은 대표 모델들의 흐름, 그리고 LLM의 한계와 과제까지 전체적으로 큰 그림을 훑을 수 있어서 좋았다. 
AI가 코드를 얼마나 "이해하는가"에 대한 생각도 해보는 시간이 되어 의미있었다고 생각한다. 

---

## 2. 예술과 인지과학
### 🐾 강의자 정보
- 교수님 : 김성대 교수님
- 소속 : 홍익대학교
- 이메일 : kimsd@hongik.ac.kr

### 🐾 _"예술이 아름답다"_ 라고 느낄 때 cycle
1. knowledge-meaning (문맥, content 등)
2. sensor-motor (감각, 지각)
3. emotion valuation

### 🐾 **신경미학 (neuroaesthetics)** 이란?
- 예술 감상 과정에서 인간의 뇌가 어떻게 작동하는지를 과학적으로 탐구하는 학문
- 미학(aesthetics) + 신경과학(neuroscience)의 융합 학문

### 🐾 내측 안와전두피질?
<span style="background-color: #fff3cd">**내측 안와 전두피질** </span>은 아름다운 예술 작품을 감상할 때 공통적으로 활성화되는 뇌의 영역이다. 

> 도덕적인 미도, 신체적인 미도 모두 내측 안와 전두 피질의 활동을 유도할 수 있다!
> <span style="color: blue">Then can be said..
매력적인 얼굴은 윤리적이고 못생긴 얼굴은 부도덕하다!? 🫢 </span>

### 🐾 섬피질?
<span style="background-color: #fff3cd">**섬피질** </span>은 불쾌한 감정이나 통증 등에 활성화되는 뇌의 영역으로, 내측 안와전두피질과 반대된다고 보면 된다. 

### 🐾 맥락효과
#### 교수님의 실험?
강의 중에 교수님께서 흥미로운 실험을 하나 하셨다.

<p align="center">
  <img src="https://medias.artmajeur.com/standard/14425985_img-0528.jpg?v=1738995421" alt="cg 미술작품1" width="300"/>
  <img src="https://medias.artmajeur.com/standard/15444652_untitled-oleo-16-x-20-9.jpg?v=17386967300" alt="cg 미술작품2" width="300"/>
</p>

_왼쪽 그림은 네덜란드의 유명 화가가 그린 작품으로 갤러리에서 전시 중인 그림이라고 소개되었고,  
오른쪽은 CG로 제작된 이미지라고 들었다. 당신은 어떤 쪽이 더 아름답다고 느껴지는가?_

수업 당시, 나를 포함한 많은 학생들이 왼쪽 그림이 더 아름답다고 손을 들었는데, 사실 **두 그림 모두 CG로 제작된 이미지**였다.

사람들은 **"화가가 직접 그렸다"는 정보(맥락)**를 들은 상태에서 이미지를 감상할 때  
**내측 안와전두피질(medial orbitofrontal cortex, mOFC)**의 활동이 **실제로 더 증가**한다고 한다.

#### 맥락효과의 정의
- **맥락효과(Context Effect)**란, 동일한 자극이라 하더라도 **그 자극이 주어지는 배경이나 정보**에 따라 사람의 인지, 감정, 판단이 달라지는 심리·인지학적 현상이다.
- 예술 감상에서도, “누가 그렸는가”, “어디서 전시되는가”와 같은 **사회적·문화적 정보**가 감정 반응과 뇌 활성에 큰 영향을 준다.

#### 배외측 전전두피질?
타인의 의견이나 맥락에 **쉽게 흔들리지 않는 ** 사람들이 있다. 
이들은 배외측 전전두피질이 더 활성화 된다고 한다.
즉, “남들이 아름답다고 하니까”가 아니라, **자기 주도적으로 미적 판단을 내리는 뇌 활동이 따로 존재**한다는 점에서 예술 감상은 개인의 정체성과도 깊이 연결된다는 사실을 보여준다.

### 🐾 부정적 감정을 동반하는 아름다움이라면?
예술 감상 중에는 꼭 긍정적인 감정만 느껴지는 것이 아니다.  
슬픔, 두려움, 고통과 같은 <u> **부정적인 감정**을 동반하면서도 **아름다움을 느끼는 경험**</u>이 있다.  
이를 대표하는 예가 **‘숭고함(sublime)’**과 **비애미(tragic beauty)**다.

#### "숭고함"을 느낄 때 활성화 되는 영역
- **미상핵 앞부분** (caudate head): 쾌감, 보상 관련
- **피각 (putamen)**, **해마 후부** (posterior hippocampus): 공포 관련

#### 예 : 미켈란젤로의 피에타 (Pietà)
![피에타](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Michelangelo%27s_Pieta_5450_cropncleaned_edit.jpg/1200px-Michelangelo%27s_Pieta_5450_cropncleaned_edit.jpg)
> 예술사에서 ‘비애미’와 ‘환희미’를 모두 느낄 수 있는 대표적 작품
- 성모 마리아가 죽은 예수를 품에 안은 **피에타**는 보는 이에게 **슬픔과 동시에 경건한 아름다움**을 안겨준다.
- 비애미를 느낄 때 :
    - **보조운동영역**과 **중간 대상회**가 활성화
    - 이 부위는 **내 고통에는 반응하지 않지만**,**타인의 고통을 공감할 때 활성화**되는 대표적인 **대리감정 회로**
- 환희미를 느낄 떄 : 
    - **내측 전전두피질** 활성화

### 🐾 짧은 소감 ?
내 전공 관련 강의는 아니었지만, 코위크 기간 중에  들은 강의 중 가장 인상깊었고 흥미로운 강의였다. 
강의를 듣고나서, 예술과 인지과학을 **인공지능과 연결해 연구해보고 싶다**는 생각도 들었다.
예를 들어 _‘AI가 만든 것’이라는 맥락을 알고 있는 것과 모를 때, 사람들의 뇌는 다른 반응을 보일까?_, _인공지능이 작곡한 음악과 인간이 작곡하고 연주한 음악을 각각 들을 때 실제 뇌에서 활성화 되는 영역 및 정도는 어떻게 차이가 날까?_ ...  이런 질문들이 자연스럽게 떠오른 재밌는 강의였다. 


---
## 3. 기초 모델의 이해와 응용 : 데이터 부족의 극복과 확장된 활용
### 🐾 강의자 정보
- 교수님 : 김태형 교수님
- 소속 : 서울대학교
- 이메일 : taehyeong.kim@snu.ac.kr

### 🐾 문제
- **현실에서의 라벨링된 데이터 얻기** → 어렵다
- 라벨링 되지 않은 데이터 라벨링하기 → 노동력 과다

### 🐾 해결
1. 자기지도학습 (Self-Supervised Learning)
- 데이터 내부 구조나 특성을 활용해 라벨 없이도 학습 가능

2. 반지도학습 (Semi-Supervised Learning)
- 소량의 라벨 데이터 + 대량의 비라벨 데이터 함께 학습

3. 전이학습 Transfer Learning / Foundation Model 활용
- pretrained 모델을 활용
- 이미 방대한 데이터로 사전학습된 모델을 가져와, **새로운 태스크에 맞게 조금만 추가 학습(fine-tuning)** 함

### 🐾 Foundation 모델
- 방대한 데이터로 사전학습된 기초 모델  
- 다양한 downstream task에 전이 가능
- 대표적 기초모델: GPT, BERT, CLIP, Segment Anything, CodeT5 등

### 🐾 Vision & Multimodal AI와 기초모델
#### 이미지 생성 모델 (ex. NeRF, diffusion)
- 기존 이미지로 pre-trained 된 모델이 새로운 이미지 생성
- 이 모델을 다른 이미지 생성 task에 전이학습 O

#### CLIP (Contrastive Language-Image Pre-training)
- text와 image 간 관계성 모델링한 연구
- 4억 개 이미지와 해당 설명 텍스트(pair)를 학습데이터로 사용
- 이미지와 텍스트 각각 인코더로 임베딩  
- 같은 pair는 임베딩 거리를 가깝게, 다른 pair는 멀게  
- **cosine similarity + cross entropy loss** 사용 (contrastive learning)

#### Segment Anything
- "모든 걸 세그먼트하는 모델"  
- 수많은 이미지와 마스크로 pre-trained된 후, 다양한 비전 task에 바로 활용되는 범용기초모델

### LLM (GPT 등)도 같은 구조!
- GPT는 **Self-supervised 방식으로 사전학습된 언어 기초모델**
- 이후 전이학습 없이도 zero-shot, few-shot으로 문제 해결 가능 → **전형적인 foundation model 구조**

### 정리
| 개념 | 설명 | 예시 |
|------|------|------|
| 사전학습 | 비라벨 대규모 데이터로 기초 능력 학습 | GPT, CLIP의 pretraining |
| 전이학습 | 학습된 모델을 다른 문제에 맞게 조정 | 감정 분석, 이미지 분류 |
| 기초모델 | 다양한 task에 활용 가능한 범용 모델 | GPT, BERT, CLIP, Segment Anything |


---
## 4. 항공드론 인공지능을 위한 강화학습
### 🐾 강의자 정보
- 교수님 : 고영민 교수님
- 소속 : 경북대학교 로봇공학과

### 🐾 강화학습 개요
- 강화학습이란?
    -  스스로 여러가지 행동을 해보고 좋은 행동(높은 보상을 받은 행동)을 찾는 것
- 강화학습이 로봇 제어, 게임 등에서 많이 사용되는 이유 :
    - 지도학습은 정확한 라벨이 필요한데, 위 문제들은 정확한 라벨 만들어 내기 어렵기 때문
- 일반적인 강화학습 시나리오 : 
    1. Agent가 초기 state S0 관측 (ex. 장애물 위치, 플레이어 위치)
    2. S0 기반으로 취할 action A0 결정 (ex. 이동방향)
    3. 해당 action의 결과로 environment 로부터 reward R1을 받음 (점수)
    4. 또한 action을 취했기 때문에 변화한 state S1을 관측
    5. 반복
- 강화학습의 목표
    - 강화학습 모델을 이와 같은 데이터에서 학습시켜, 최고의 정책을 찾는 것
    - 정책 (π(a|s)) : 상태(state)가 주어졌을 때 어떤 행동 (action)을 취할 지 결정하는 방법
     

### 🐾 강화학습 기본 이론
#### 확률?
- **확률 p(X = x)**: 확률 변수 X가 특정 값 x을 가질 확률
- **결합확률 P(X, Y)**: 두 사건이 동시에 일어날 확률 (두 사건이 독립인 경우)
- **조건부확률 P(X|Y)**: Y가 발생했을 때 X가 일어날 확률
- **기댓값 E(P(X))**: 기대되는 보상의 평균
<u> Agent의 행동, environment가 주는 보상 등이 확률적인 경우가 많다 ! </u>

#### 정책의 가치
- **이득** : 앞으로 얻을 것으로 예상되는 reward의 합
    - _Gt = Rt+1 + Rt+2 ... RT_
    - T는 종단상태
        - 종단 상태가 존재하지 않아서 시나리오가 무한히 지속되는 경우도 O -> 이때 이득값은 발산
- **Discount rate** : 먼 미래의 보상보다는 가까운 미래의 보상에 가중치를 줌
    - 0~1 사이의 값
    - 이득값 발산 방지
- 정책 π: 상태 s에서 어떤 행동 a를 할 확률 분포 (π(a|s))
- 가치함수 vπ(s): 현재의 상태 s에서 정책 π를 따랐을 때 받을 것으로 기대되는 누적 보상
    - _vπ(s) = Eπ[Gt | St = s]_
    - 벨만 방정식 : 가치함수를 재귀적으로 표현함
        - _vπ(s) = Σa π(a|s) Σs',r p(s',r|s,a)[r + γvπ(s')]_

#### 정책 반복 및 평가
- 초기 정책(무작위) -> 정책 평가 -> 정책 개선 -> 반복
- 가치가 수렴할 때까지 반복하여 최고의 정책 도출
- 정책 개선할 때는 현재 정책의 가치 v(s)를 계산한 뒤, 더 높은 가치를 주는 행동으로 이동하는 방식 (greedy)


### 🐾 딥러닝과 강화학습
#### 머신러닝
- 데이터셋을 통해 학습해서, 원하는 결과를 내기 위한 최적의 파라미터 찾아내기
- 목표 : 데이터를 설명할 수 있는 최적의 모델(함수) 찾기

#### 딥러닝
- 신경망 기반의 머신러닝 확장이라고 보면 됨
    - 여러 층을 가진 네트워크로 복잡한 패턴 학습 O
    - 각 층은 가중치 w와 bias 학습해서 표현함
- DQN (Deep Q-Network)
    - 기존에는 벨만 방정식을 이용해 반복계산을 수행했음
    - DQN은 <u>신경망을 통해 정책을 표현</u> 하고, 이를 안정적으로 최적화하기 위한 여러 방법을 제시함
    - 사실 이 부분 잘 기억 안나고 이해 못한 것 같아요..ㅎ 정리 추후에 다시 하겠습니다. 껄껄..

<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}