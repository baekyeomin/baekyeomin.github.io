---
title: "[ML] 4. 로지스틱 회귀(Logistic Regression)"
categories: [AI, ML]
tags:
  - 머신러닝
  - 지도학습
  - 회귀
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}


## 이진 분류 문제
**예시**
- 이메일 : 스팸인가 / 아닌가
- 온라인 거래 : 사기인가 / 아닌가
- 종양 : 악성(malignant)인가 / 양성(benign)인가
<br>
<br>

**로지스틱 회귀**
- <span style="color: blue">선형 회귀</span>의 출력은 $(-\infty,\infty)$라 분류에 적합하지 않음
- <span style="color: blue">로지스틱 회귀</span>의 출력은 <span style="font-size:200%"> $y \in \{0,1\}$ </span>
- <span style="background-color: #fff3cd">로지스틱 회귀는 **시그모이드(sigmoid)** 함수로 $\theta^Tx$를 0~1사이 값 매핑이 가능하게 함 </span> -> <span style="color: blue">확률처럼 해석 가능 ! </span>
<br>
<span style="font-size:200%"> $h_\theta(x)=g(\theta^Tx),\quad g(z)=\frac{1}{1+e^{-z}}$ </span> <br>
  <br>
  <br>
  <br>

## 가설함수
<span style="font-size:200%"> $h_\theta(x)=g(\theta^Tx),\quad g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$ </span> <br>

- 시그모이드 함수의 e의 지수를 linear 모델로 대체한 형태이다. 
- **선형모델의 한계 보완** :
  선형회귀처럼 $\theta^Tx$ 를 쓰되(구조유지), **시그모이드**로 감싸 **출력을 확률처럼** 나타냄

![linear regression graph](/assets/images/sigmoid.png)  

  <br>
  <br>
  <br>

##  Decision Boundary(결정 경계)
### 🐾 정의 
분류 문제에서, **예측값이 0과 1 사이에서 경계가 되는 선/곡선**
-><span style="color: blue"> 여기까진 클래스 0, 저기부턴 클래스 1을 나누는 선 ! </span>

<br>
<span style="font-size:200%"> $ h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} $ </span>
- 보통 기준으로 삼는 수 = 0.5
- 따라서,<span style="font-size:150%"> $ h_\theta(x) = 0.5 , \theta^Tx = 0 $</span>
  <span style="background-color: #fff3cd">"결정 경계"는 **$h_\theta(x)=0.5$가 되는 지점**을 의미하며,  
  이 값은 $\theta^Tx=0$과 동일</span>


### 🐾 선형 결정 경계
예를 들어, 입력 특징이 2개 ($x_1, x_2$)라면:<br>
<span style="font-size:200%"> $ h_\theta(x) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2) $</span>
- 이때, 결정 경계는<span style="font-size:150%"> $ \theta_0 + \theta_1 x_1 + \theta_2 x_2 = 0 $ </span>
  
### 🐾 비선형 결정 경계
현실 데이터는 직선으로 나누기 어려운 경우가 많으니, 입력 특징을 feature mapping 해서 비선형 항을 추가하자!<br>

- $x_1^2$, $x_2^2$ 같은 항을 추가하면 **원형, 곡선, 타원** 형태의 경계도 학습할 수 있음.
- 즉, 단순히 직선이 아니라 **비선형적인 결정 경계**까지 표현 가능.

  <br>
  <br>
  <br>

##  비용함수
### 🐾 MSE(제곱오차) 사용의 문제
로지스틱 회귀에 MSE를 쓰면, $J(\theta)$가 비볼록(non-convex)하게 되어 경사하강법이 local minimum되거나 진동(발산)하게 될 수 있음

### 🐾 크로스 엔트로피
로지스틱 회귀에서는 예측값과 실제값의 차이를 단순 제곱 오차 (MSE)로 측정하지 않고, **크로스엔트로피 손실**을 사용함<br>
<br>
<span style="color: blue">한 개의 데이터 $(x, y)$에 대해 비용</span>은 다음과 같이 정의된다 : <br>
$\text{cost}(h_\theta(x), y) = \begin{cases}
-\log(h_\theta(x)), & y=1 \\
-\log(1-h_\theta(x)), & y=0
\end{cases}$

- 만약 **실제 정답이 1**인데, 모델이 $h_\theta(x) \to 1$이라고 예측하면  
  $\log(1)$이 되어 비용이 **0에 가까움 -> 잘 맞춘 것**  
- 반대로, **실제 정답이 1**인데 $h_\theta(x) \to 0$이라고 예측하면  
  $-\log(0)$이 되어 비용이 커짐 -> 예측 잘 못한 것
<br>
<span style="color: blue"> 전체 비용함수 </span> <br>
m개의 학습 샘플에 대해 평균 비용을 계산하면 다음과 같다. <br>
$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m}
\Big[ y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log \big(1 - h_\theta(x^{(i)}) \big) \Big]
$
- $y^{(i)} = 1$인 경우: $-\log(h_\theta(x^{(i)}))$ 만 남음  
- $y^{(i)} = 0$인 경우: $-\log(1-h_\theta(x^{(i)}))$ 만 남음  
- 즉, 한 줄 수식으로 **$y$ 값에 따라 자동으로 맞는 항을 선택**하도록 합쳐놓은 것.
<br> 
  <br>
  <br>

##  경사하강법
로지스틱 회귀의 비용함수를 최소화하는 세타값 찾기<br>
-> HOW? 경사하강법으로!<br>
<br>
<u>비용 함수 $J(\theta)$를 $\theta_j$에 대해 편미분하면: </u> <br>

<span style="font-size:200%">$
\frac{\partial J(\theta)}{\partial \theta_j}
= \frac{1}{m} \sum_{i=1}^{m} \big( h_\theta(x^{(i)}) - y^{(i)} \big) x_j^{(i)}
$ </span>

<br>
<u>경사하강법은 파라미터 $\theta$를 반복적으로 다음과 같이 갱신한다. </u>
<span style="font-size:200%">$
\theta_j := \theta_j - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m} \big(h_\theta(x^{(i)}) - y^{(i)}\big)x_j^{(i)}$</span>
- $\alpha$: 학습률 (learning rate)  
- $m$: 샘플 개수 
   
<br>
<u>모든 $\theta$를 동시에 업데이트하는 벡터화 식은:</u>

$\theta := \theta - \alpha \cdot \frac{1}{m} X^T (h_\theta(X) - y)$

여기서  
- $X$: 입력 행렬 ($m \times n$)  
- $h_\theta(X)$: 전체 샘플에 대한 예측 벡터 
    <br>
  <br>
  <br>

##  Advanced Optimization
최적화 알고리즘에는 다양한 것이 있다. <br>
예 : **Conjugate Gradient**, **BFGS**, **L-BFGS**
<br>
- **장점**
  - **학습률을 직접 정할 필요 X**
  - 경사하강법보다 **적은 반복으로 수렴** → 더 빠른 수렴이 가능하고 대용량 데이터에서도 유리
- **단점**
  - 구현이 복잡하지만, `scipy.optimize` 등의 라이브러리로 손쉽게 사용 가능

  <br>
  <br>
  <br>

##  다중클래스 분류 (Multi-class classification : One vs All)
**예시**
- 날씨 : 흐림/맑음/비/눈
- 질병 : 정상 / 감기 / 독감...
- email foldering : work / friend / family / hobby
<br>
<br>
클래스 수가 k개일 때 클래스마다 이진 로지스틱 분류기 학습시키는 것. <br>

### 🐾 방법 
<u> 입력 x가 class i일 확률을 구하는 것 </u>
<br>
<br>
각 class마다 이진 분류 모델 학습하고 가장 큰 확률의 클래스를 선택하는 것이다. <br>
<span style="font-size:200%">$\hat y = \arg\max_{i \in \{1,\dots,K\}} h_\theta^{(i)}(x)$ </span>

<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}