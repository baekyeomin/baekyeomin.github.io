---
title: "[ML] 4. 로지스틱 회귀(Logistic Regression)"
categories: [AI, ML]
tags:
  - 머신러닝
  - 지도학습
  - 회귀
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}


## 이진 분류 문제
**예시**
- 이메일 : 스팸인가 / 아닌가
- 온라인 거래 : 사기인가 / 아닌가
- 종양 : 악성(malignant)인가 / 양성(benign)인가
<br>
<br>

**로지스틱 회귀**
- <span style="color: blue">선형 회귀</span>의 출력은 $(-\infty,\infty)$라 분류에 적합하지 않음
- <span style="color: blue">로지스틱 회귀</span>의 출력은 <span style="font-size:200%"> $y \in \{0,1\}$ </span>
- <span style="background-color: #fff3cd">로지스틱 회귀는 **시그모이드(sigmoid)** 함수로 $\theta^Tx$를 0~1사이 값 매핑이 가능하게 함 </span> -> <span style="color: blue">확률처럼 해석 가능 ! </span>
<br>
<span style="font-size:200%"> $h_\theta(x)=g(\theta^Tx),\quad g(z)=\frac{1}{1+e^{-z}}$ </span> <br>
  <br>
  <br>
  <br>

## 가설함수
<span style="font-size:200%"> $h_\theta(x)=g(\theta^Tx),\quad g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$ </span> <br>

- 시그모이드 함수의 e의 지수를 linear 모델로 대체한 형태이다. 
- **선형모델의 한계 보완** :
  선형회귀처럼 $\theta^Tx$ 를 쓰되(구조유지), **시그모이드**로 감싸 **출력을 확률처럼** 나타냄

![linear regression graph](/assets/images/sigmoid.png)  

  <br>
  <br>
  <br>

##  Decision Boundary(결정 경계)
### 🐾 정의 
분류 문제에서, **예측값이 0과 1 사이에서 경계가 되는 선/곡선**
-><span style="color: blue"> 여기까진 클래스 0, 저기부턴 클래스 1을 나누는 선 ! </span>

<br>
<span style="font-size:200%"> $ h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} $ </span>
- 보통 기준으로 삼는 수 = 0.5
- 따라서,<span style="font-size:150%"> $ h_\theta(x) = 0.5 , \theta^Tx = 0 $</span>
  <span style="background-color: #fff3cd">"결정 경계"는 **$h_\theta(x)=0.5$가 되는 지점**을 의미하며,  
  이 값은 $\theta^Tx=0$과 동일</span>


### 🐾 선형 결정 경계
예를 들어, 입력 특징이 2개 ($x_1, x_2$)라면:<br>
<span style="font-size:200%"> $ h_\theta(x) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2) $</span>
- 이때, 결정 경계는<span style="font-size:150%"> $ \theta_0 + \theta_1 x_1 + \theta_2 x_2 = 0 $ </span>
  
### 🐾 비선형 결정 경계
현실 데이터는 직선으로 나누기 어려운 경우가 많으니, 입력 특징을 feature mapping 해서 비선형 항을 추가하자!<br>

- $x_1^2$, $x_2^2$ 같은 항을 추가하면 **원형, 곡선, 타원** 형태의 경계도 학습할 수 있음.
- 즉, 단순히 직선이 아니라 **비선형적인 결정 경계**까지 표현 가능.

  <br>
  <br>
  <br>

##  비용함수
### 🐾 MSE(제곱오차) 사용의 문제
로지스틱 회귀에 MSE를 쓰면, $J(\theta)$가 비볼록(non-convex)하게 되어 경사하강법이 local minimum되거나 진동(발산)하게 될 수 있음

### 🐾 크로스 엔트로피
로지스틱 회귀에서는 예측값과 실제값의 차이를 단순 제곱 오차 (MSE)로 측정하지 않고, **크로스엔트로피 손실**을 사용함<br>
<br>
한 개의 데이터 $(x, y)$에 대해 비용은 다음과 같이 정의된다 : <br>
$$
\text{cost}(h_\theta(x), y) =
\begin{cases}
-\log \big(h_\theta(x)\big), & y=1 \\
-\log \big(1 - h_\theta(x)\big), & y=0
\end{cases}
$$
<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}