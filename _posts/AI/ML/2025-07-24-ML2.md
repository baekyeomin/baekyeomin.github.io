---
title: "[ML] 2. 단일변수 선형회귀 (Linear Regression with One Variable) "
categories: [AI, ML]
tags:
  - 머신러닝
  - 지도학습
  - 회귀
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}


## 선형 회귀 모델
### 🐾 정의
<span style="background-color: #fff3cd">입력 변수(특징, feature)와 출력 변수 간의 관계를 **직선(선형 함수)**으로 근사해 연속적인 값을 예측하는 방법 </span>
- 예 : 집 크기에 따른 집값 예측 



![linear regression graph](/assets/images/linear_regression_example.png)  

위 그림은 집 면적(평방피트)와 집값(천 달러 단위)의 관계를 나타낸 그림이다. 
- 각 점 (x표)은 학습 데이터, 보라색 선은 모델이 학습한 **예측 함수(hypothesis function)**
- $x^{(i)}$ = i번째 입력값 
- $y^{(i)}$ = i번째 출력값 
- $(x^{(i)}, y^{(i)})$ = i번째 학습 샘플


### 🐾 가설함수
선형 회귀 모델의 가설 함수는 다음과 같다 : 
<span style="font-size:150%"> $h_\theta(x) = \theta_0 + \theta_1$ </span>

- $\theta_0$: y절편 (bias)  
- $\theta_1$: 기울기 (weight)  

<br> 
<br> 
<br> 

## 비용 함수 (Cost Function)
모델의 성능을 평가하기 위해 <span style="background-color: #fff3cd">**비용 함수 J(θ)**</span>를 사용한다.
<br> 가장 많이 쓰는 함수는 **평균제곱오차(MSE)** 다.

$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)^2
$

<span style="background-color: #fff3cd">비용 함수 값을 최소화하는 $\theta_0$와 $\theta_1$을 찾는 것</span>이 최적화의 목표이며,  
이를 통해 데이터에 가장 잘 맞는 직선을 구할 수 있다. 이때 이러한 최소화를 수행하기 위해 사용하는 대표적인 방법이 **경사하강법(Gradient Descent)**이다.

> <span style="color: blue">손실함수</span>와 <span style="color: blue">비용함수</span>가 공부하면서 헷갈렸어서 정리함! <br> 
> - **손실 함수 (Loss Function)**  
>   : _하나_ 의 학습 샘플에 대한 오차를 계산하는 함수  
>   $ L\big(h_\theta(x^{(i)}), y^{(i)}\big) = (h_\theta(x^{(i)}) - y^{(i)})^2 $
>
> - **비용 함수 (Cost Function)**  
>   : _전체_ 학습 데이터에 대한 손실 함수의 평균 (or 합)  
>   $ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \big(h_\theta(x^{(i)}) - y^{(i)}\big)^2 $
>
> 정리하면:  
> - 개별 데이터 → **Loss**  Function
> - 전체 데이터 평균 → **Cost** Function


### 🐾 Intuition
- 직선의 기울기($\theta_1$)나 절편($\theta_0$) 값에 따라, 데이터와 직선 사이의 오차가 달라진다. (사진참고)
- 이 오차를 전체 데이터에 대해 계산해 나타낸 것이 비용 함수 $J(\theta)$이다.  

![linear regression graph](/assets/images/linear_regression_costf_example.png)  


### 🐾 비용함수 도함수 유도
비용함수 공식 :  $ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \big(h_\theta(x^{(i)}) - y^{(i)}\big)^2 $

도함수 계산 : 
- $\theta_0$에 대한 편미분
$
\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) 
= \frac{1}{m}\sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)
$

- $theta_1$에 대한 편미분
$
\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) 
= \frac{1}{m}\sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)\, x^{(i)}
$
<br>
.
.
.
수렴할 때까지 반복

<br> 
<br> 
<br> 

## 경사하강법 (Gradient Descent)
비용 함수를 *최소화*하여 가장 잘 맞는 모델(선)을 찾기 위한 방법

### 🐾 과정
1. 초기 파라미터 값 설정 (아무거나)
2. 기울기 계산
3. 파라미터 값 업데이트 -> 비용이 줄어드는 방향을 따라감
4. 수렴할 때까지 반복


### 🐾 수식
$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)
$

> 여기서 := 는 **대입** 을 의미함 ! <br> 수학에서의 = 와는 다름 <br>
> $\alpha$: <span style="background-color: #fff3cd">학습률 (learning rate) </span> , 얼마나 크게 움직일지
  

### 🐾 학습률
- 학습률 $\alpha$가 너무 **작으면** : 수렴 속도 느려지거나 <span style="color: blue">local minimum </span> 문제
- 학습률 $\alpha$가 너무 **크면** : 발산 가능
- 적절한 학습률이 설정 시 $J(\theta)$가 매 반복마다 감소하다 수렴

### 🐾 지역 최솟값 vs 전역 최솟값
- 손실 함수(또는 비용 함수)가 **convex(볼록)**이면, **local minimum 문제는 발생하지 않는다.**  
- 따라서 선형 회귀의 비용 함수는 convex 구조를 가지므로, 경사하강법을 사용하면 항상 전역 최솟값에 수렴한다.  
- 반대로 신경망과 같은 비선형 모델의 비용 함수는 **non-convex** 형태이므로, local minimum 발생 가능

 
<p align="center">
  <img src="/assets/images/gradient_descent.png" width="45%" />
  <img src="/assets/images/gradient_descent2.png" width="45%" />
</p>

<br> 
<br> 
<br> 

## 다양한 경사하강법  
### 🐾 배치 경사하강법 (Batch Gradient Descent)
- <span style="background-color: #fff3cd">모든 학습 데이터를 한꺼번에 사용해서 기울기(gradient)를 계산하고, 한 번 업데이트. </span> 
  - 즉, m개의 샘플을 다 합쳐서 평균 오차 방향을 구함 → 그 방향으로 $\theta\$를 조정  
- 장점 :
  - 수렴이 안정적, 정확도가 높음
- 단점 :
  - 데이터가 크면 계산량이 커져서 비효율적임
  - 속도가 느릴 수 있음

  
### 🐾 확률적 경사하강법 (Stochastic Gradient Descent, SGD)
- <span style="background-color: #fff3cd">한 번에 **하나 혹은 일부 훈련 샘플**만을 사용</span>하여 기울기를 계산하고 업데이트
- 장점 :
  - 계산 비용 절감
  - 대규모 데이터셋에도 O
- 단점 :
  - 매 업데이트마다 일부 데이터만 반영하기에 기울기 방향이 불안정하고, 노이즈가 많아서 들쭉날쭉할 수 있음. 
  - 수렴 속도가 느려지거나 local minimum 도달 어려울 수도
  <br>
- <u>단점 보완한 알고리즘</u>
  - **모멘텀(Momentum)**  
    - 이전 단계의 기울기를 일정 비율 반영하여 관성을 부여함
    - 불필요한 진동을 줄이고 더 빠른 수렴 가능  

  - **AdaGrad**  
    - 파라미터마다 학습률을 동적으로 조정함
    - 자주 업데이트되는 파라미터는 학습률을 줄이고, 드물게 업데이트되는 파라미터는 학습률을 크게 유지  

  - **이외에도 Adam 등이 있다고 한다 ~** 


### 🐾 미니배치 경사하강법 (Mini-batch Gradient Descent)
- <span style="background-color: #fff3cd">전체 데이터를 작게 나눈 **배치(예: 32개, 64개, 128개 샘플)** 단위로 나눠서</span> 기울기를 계산하고 업데이트하는 방법
- 장점 : 
  - 계산 효율성 조음
  - 안정적
- 단점 : 
  - 배치 크기를 잘 정해야함 (너무 크면 Batch처럼 느려지고 너무 작으면 SGD처럼 진동 심해질수도)
  

### 🐾 요약

| 방법 | 한 번에 쓰는 데이터 | 장점 | 단점 | 비유 |
|------|-----------------|------|------|------|
| **Batch** | 전체 데이터 | 수렴 안정적, 이론적 분석 용이 | 느림, 대규모 데이터 비효율적 | 전교생 점수 평균 내기 |
| **SGD** | 1개 샘플 | 빠름, 빅데이터 적합 | 진동 큼, 불안정 | 학생 1명만 보고 판단 |
| **Mini-batch** | 일부 샘플 (32~256) | 속도·안정성 균형, 실무 표준 | 배치 크기 선택이 중요 | 반 학생들 점수 평균 내기 |

![경사하강법들](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS0HbwglKYFGLozH4GBN69CTlocP5y6c5S0Og&s)

<br>
출처 : https://didalsgur.tistory.com/entry/SGD-Batch-miniBatch-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95-%EB%B9%84%EA%B5%90