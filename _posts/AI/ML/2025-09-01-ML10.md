---
title: "[ML] 10. Maximum Likelihood Estimation (MLE) & EM Algorithm"
categories: [AI, ML]
tags:
  - 머신러닝
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}

## Probability vs Likelihood
Probability: 모델(분포)이 주어졌을 때, 데이터가 관측될 확률 <br>
$P(x \mid \theta)$

Likelihood: 데이터가 주어졌을 때, 해당 데이터를 가장 잘 설명하는 파라미터 $\theta$ 찾기 <br>
$P(\theta \mid x)$

<br>
<br>
<br>

## 최대우도추정 (MLE: Maximum Likelihood Estimation)
- 주어진 데이터 $D = \{x_1, x_2, \dots, x_n\}$에 대해 파라미터 $\theta$를 추정하는 방법
- 목표: $\theta$가 주어졌을 때 데이터가 발생할 확률을 최대화하는 $\theta$를 찾음

<br>

- Likelihood 함수:
$L(\theta \mid x) = \prod_{i=1}^N P(x_i \mid \theta)$

- 로그우도(Log-Likelihood) 사용 (계산 편의성을 위해):
$\ell(\theta) = \log L(\theta \mid x) = \sum_{i=1}^N \log P(x_i \mid \theta)$

- 미분하여 극값 찾기:
$\frac{\partial}{\partial \theta} \ell(\theta) = 0$

### 🐾 성질
1. **대수적 정확성 (Asymptotic Consistency)**  
   표본의 크기 $N \to \infty$ 일 때, 추정량 $\hat{\theta}_{MLE}$이 참값 $\theta$에 수렴한다.

2. **대수적으로 효율적**<br>
   N이 무한히 커지면, fisher information matrix(다변수 파라미터에서 각 변수간 정보량을 담은 대칭행렬)의 역행렬 만큼의 분산을 MLE는 가짐

3. **대수적으로 정규분포성**<br>
   충분히 큰 N에서 MLE는 정규분포를 따른다. <br>
   $\sqrt{N}(\hat{\theta}_{MLE} - \theta) \sim N(0, I(\theta)^{-1})$

<br><br><br>

## EM Algorithm (Expectation Maximization)
### 🐾 kmeans와 비교
- kmeans는 각 점을 하나의 cluster에만 소속시키는 hard assignment!
- EM은 kmeans를 확률적인 관점에서 일반화한 것
- <span style="background-color: #fff3cd">각 점이 각 cluster에 속할 확률을 구함</span> -> soft assignment

|구분|K-means|EM (Expectation-Maximization)|
|---|---|---|
|**Cluster 표현**|각 클러스터를 하나의 **평균(mean)** 으로 표현|각 클러스터를 **평균(mean), 분산(공분산), 가중치**를 가진 확률분포(가우시안)로 표현|
|**Cluster 초기화**|K개의 **랜덤 평균**을 초기 centroid로 설정|K개의 **가우시안 분포** (평균, 분산, 가중치) 초기 설정|
|**예측 (할당)**|각 데이터 포인트를 **가장 가까운 centroid**에 할당|각 데이터 포인트가 **각 클러스터에 속할 확률**을 계산 (soft assignment)|
|**최대화 (업데이트)**|각 클러스터의 centroid를 데이터의 **평균으로 업데이트**|각 클러스터의 **평균, 분산(공분산), 가중치**를 업데이트|
|**특징**|단순하고 빠르지만, **하드 할당**(한 클러스터에만 속함)|확률 기반으로 **소프트 할당** 가능, 더 복잡한 데이터 분포 모델링 가능

 
### 🐾 과정
1. **초기화**  
   파라미터 $\theta^{(0)}$ 설정

2. **E-step (Expectation)**  
   현재 파라미터 $\theta^{(t)}$에서 잠재변수의 기대값 계산  
   $Q(\theta \mid \theta^{(t)}) = E_{Z \mid X, \theta^{(t)}} \big[ \log P(X, Z \mid \theta) \big]$

3. **M-step (Maximization)**  
   $Q(\theta \mid \theta^{(t)})$를 최대화하는 $\theta^{(t+1)}$ 업데이트  
   $\theta^{(t+1)} = \arg\max_\theta Q(\theta \mid \theta^{(t)})$

4. **반복**  
   수렴할 때까지 2~3단계를 반복

<br><br><br>

## Decision Tree
결정 트리 예시 그림 <br>
![결정트리예시](/assets/images/decisiontree.png) <br>
- 결정트리는 입력 속성의 어떤 Bool 함수라도 표현할 수 O
- 단, 복잡도/입력 수에 따라 노드 수가 지수적으로 늘어날 수 있다
- 가설공간의 크기가 가변적이기에 트리의 깊이나 노드수에 따라 복잡하고 다양한 함수 표현이 가능하다
![결정트리예시2](/assets/images/결정트리노트필기.png) <br>


> *오컴의 면도날*<br>
> 가장 간단한 결정트리가 최선의 선택이다!
> <br>단, 최소 트리를 찾는 것은 NP-hard 문제라 완전 탐색이 불가하다


> *ID3 기법*
> 1. A = 현재 노드에서 가장 정보 이득이 큰 특성 찾기
> 2. 선택한 A를 현재 노드의 분할 기준으로 지정
> 3. A의 각 값에 대해 새로운 자식 노드를 생성한다
> 4. training example을 각 리프 노드로 정렬한다
> 5. 리프 노드에서 example들이 모두 완벽하게 분류되면 몸추기
>
> **좋은 attribute는 데이터 분할 시, 각 서브셋이 이상적으로 전부 양성이거나 음성이 되어야함**

![결정트리3](/assets/images/결정트리노트필기2.png) <br>

### 🐾 정보이득 (information gain)
1. 엔트로피 (Entropy)

데이터의 불확실성을 측정하는 지표  

클래스가 $c_1, c_2, \dots, c_k$ 일 때:
$H(Y) = - \sum_{i=1}^k P(y_i) \log_2 P(y_i)$

- $P(y_i)$ : 클래스 $y_i$의 확률  
- 엔트로피 값이 클수록 불확실성이 큼 → 분류가 어려움  
- 엔트로피 값이 0이면 완전 분류된 상태  



2. 조건부 엔트로피 (Conditional Entropy)

어떤 속성 $X$를 기준으로 분할했을 때의 불확실성  
$H(Y|X) = \sum_{v \in Values(X)} P(X=v) H(Y|X=v)$

- $H(Y|X=v)$ : 속성 $X=v$로 분할했을 때의 엔트로피  
- 조건부 엔트로피가 작을수록 좋은 분할  


3. 정보 이득 (Information Gain)

특정 속성 $X$를 기준으로 데이터를 분할했을 때, 불확실성이 얼마나 줄어들었는지를 측정  

정의:  
$IG(Y, X) = H(Y) - H(Y|X)$

- 전체 엔트로피 $H(Y)$에서 속성 $X$로 분할한 후의 엔트로피 $H(Y|X)$를 뺀 값  
- 정보 이득이 클수록 분할 후 데이터가 잘 나뉘었다는 의미
- - ID3, C4.5, C5.0 등의 결정트리 알고리즘에서 **노드 분할 기준**으로 활용 

<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}