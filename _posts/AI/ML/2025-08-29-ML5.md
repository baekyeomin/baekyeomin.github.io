---
title: "[ML] 5. 정규화 (Regularization) "
categories: [AI, ML]
tags:
  - 머신러닝
  - 정규화
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}

## overfitting 과 underfitting
### 🐾 Overfitting
지나치게 많은 파라미터 또는 고차항을 사용해서 학습 데이터를 거의 완벽하게 맞춤 <br>
<br>
- 이러면 새 데이터에는 일반화 (generalization) 잘 안됨
- 예측 성능 낮아짐
- <span style="color: blue"> 분산 (variance)가 높음 </span>
  
<br>

### 🐾 Underfitting
단순해서 학습데이터를 잘 못 맞춤 <br>
<br>
- 예측이 부정확해짐
- <span style="color: blue"> 편향 (bias)가 높음</span>
<br>

**Underfitting vs. Good Fit vs. Overfitting**
![과적합 과소적합](https://wikidocs.net/images/page/164364/05_under_good_overfit.png)

<br>
<br>
<br>

## 정규화
파라미터들의 크기를 제한해서 모델이 너무 복잡해지는걸 막는 방법<br>즉, <span style="background-color: #fff3cd"> 모델의 복잡도에 대한 패널티 </span> <br>
<br>

### 🐾 정규화 목적
**1. 과적합 (overfitting 방지)** <br>
- 차수가 높고 파라미터 수가 많을수록 가중치(고차항의 계수)가 커져서 데이터의 <u> 노이즈까지 학습할 수 있다 </u> <br>-> <span style="color: blue">과적합 방지 위해서 필요함</span>

**2. 간단한 모델 유도** <br>
- 높은 차수의 계수를 작게 만들어서 복잡도 낮추기

**3. 일반화 성능 향상** <br>
- 새로운 데이터에 더 잘 작동할 수 있도록

> L1 L2는 수업에서 다루지 않았지만 그냥 궁금해서 찾아봤던 기억이 .. ~

> ### 🐾 L1 정규화 (L1 norm)  
> - **Norm**  
>   - 벡터의 크기, 두 벡터 간 거리 측정하는 함수  
>   
> - **L1 Norm**  
>   - 맨허튼 거리 $d = |a_1 - b_1| + |a_2 - b_2|$  
>   - $\|x\|_1 = \sum_{i=1}^{n} |x_i|$  
>   
> - **L1 손실**  
>   - $L = \sum_{i=1}^{n} |y_i - f(x_i)|$  
>   
> - **L1 정규화**  
>   - $Loss = 기존 \ Loss + \lambda \sum_j |w_j|$  
>   - 가중치를 0으로 만들 수 있음  
>   - 편미분할 때 sgn 사용  
>     $$
>       sgn(w) = 
>     \begin{cases}
>     1 & \text{if } w > 0 \\\\
>     0 & \text{if } w = 0 \\\\
>     -1 & \text{if } w < 0
>     \end{cases}
>     $$  
>   - 업데이트 식 :  
>     $w \leftarrow w - \frac{\eta \lambda}{n} \cdot sgn(w) - \eta \cdot \frac{\partial C_0}{\partial w}$  

> ### 🐾 L2 정규화 (L2 norm)  
> - **L2 Norm**  
>   - 유클리드 거리 ($ = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2}$)  
>   - $\|x\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}$  
>   
> - **L2 손실**  
>   - $L = \sum_{i=1}^{n} (y_i - f(x_i))^2$  
>   
> - **L2 정규화**  
>   - $Loss = 기존 \ Loss + \lambda \sum_j w_j^2$  
>   - 가중치를 0에 가깝게 만듦 (0은 아님)  
>   - 편미분할 때 선형적으로 감소함  
>     $w \leftarrow \left(1 - \frac{\eta \lambda}{n} \right) w - \eta \cdot \frac{\partial C_0}{\partial w}$  
>   
> ![정규화](https://blog.kakaocdn.net/dna/bENf8W/btrDXowms1A/AAAAAAAAAAAAAAAAAAAAAJl5YWJie9Uii1mukWC5aL2nzryNeHbkm4Albk_y8D6o/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1756652399&allow_ip=&allow_referer=&signature=S6CN9MArrlY4L70yiT3TCc3CqVw%3D)  
> 출처 ㅣ https://seongyun-dev.tistory.com/52


### 🐾 정규화된 선형회귀
#### 기존 선형회귀의 비용함수 (MSE)
$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 $


#### 정규화된 선형회귀의 비용함수
$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2$

- $\lambda$는 정규화 계수 (hyperparameter)
  - $\lambda$가 클수록 $\theta_j^2$ 작게 만드려는 것
  - <span style="color: blue">만일 $\lambda$가 너무 크면? (예 : $\lambda = 10^10$) </span>
    - overfitting 제거 실패, underfitting 발생? -> $\lambda$가 너무 커서 모든 $\theta_j$가 작아짐
    - 경사하강법 수렴 X -> $\lambda$가 너무 커서 cost function 너무 평평하거낭...
    - 세타 값들을 0에 너무 강제로 묶여둠 -> 학습 거의 안되는 모델이 되어 좋은 성능 못 냄
- - <span style="color: blue">만일 $\lambda$가 너무 작으면? (예 : $\lambda = 10^10$) </span>
  - overfitting 문제 해결 X
  - 훈련데이터에만 맞춰짐
- $\theta_0$ (bias term)는 정규화 대상에서 제외


<br>

### 🐾 정규화된 경사하강법 (Gradient Descent with Regularization)
Repeat { <br>
    $\theta_0 := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_0^{(i)}$<br>
    $\theta_j := \theta_j - \alpha \cdot \left[ \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)} + \frac{\lambda}{m} \theta_j \right]$<br>
}

<br>

### 🐾 정규화된 정규방정식 (Regularized Normal Equation) 
이 부분은 수업 안 하고 넘어가서 사실 잘 몰라욤..
$\theta = \left( X^T X + \lambda \tilde{I} \right)^{-1} X^T y$


### 🐾 정규화된 로지스틱 회귀
#### 시그모이드 함수
$h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}$

#### 정규화된 로지스틱 회귀의 비용함수 
$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$

- 두 번째 항이 정규화 항
- $\theta_0$는 정규화에서 제외됨

#### 정규화된 경사하강법
Repeat { <br>
    $\theta_0 := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}$<br>
    $\theta_j := \theta_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} + \frac{\lambda}{m} \theta_j \right]$<br>
}

<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}