---
title: "[ML] 5. ì •ê·œí™” (Regularization) "
categories: [AI, ML]
tags:
  - ë¨¸ì‹ ëŸ¬ë‹
  - ì •ê·œí™”
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

êµ­ë¯¼ëŒ€ ê¹€ì¥í˜¸ êµìˆ˜ë‹˜ "ë¨¸ì‹ ëŸ¬ë‹ê¸°ì´ˆ" ê³¼ëª©(2í•™ë…„ 1í•™ê¸° ìˆ˜ê°•)ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.
{: .notice--info}

## overfitting ê³¼ underfitting
### ğŸ¾ Overfitting
ì§€ë‚˜ì¹˜ê²Œ ë§ì€ íŒŒë¼ë¯¸í„° ë˜ëŠ” ê³ ì°¨í•­ì„ ì‚¬ìš©í•´ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ê±°ì˜ ì™„ë²½í•˜ê²Œ ë§ì¶¤ <br>
<br>
- ì´ëŸ¬ë©´ ìƒˆ ë°ì´í„°ì—ëŠ” ì¼ë°˜í™” (generalization) ì˜ ì•ˆë¨
- ì˜ˆì¸¡ ì„±ëŠ¥ ë‚®ì•„ì§
- <span style="color: blue"> ë¶„ì‚° (variance)ê°€ ë†’ìŒ </span>
  
<br>

### ğŸ¾ Underfitting
ë‹¨ìˆœí•´ì„œ í•™ìŠµë°ì´í„°ë¥¼ ì˜ ëª» ë§ì¶¤ <br>
<br>
- ì˜ˆì¸¡ì´ ë¶€ì •í™•í•´ì§
- <span style="color: blue"> í¸í–¥ (bias)ê°€ ë†’ìŒ</span>
<br>

**Underfitting vs. Good Fit vs. Overfitting**
![ê³¼ì í•© ê³¼ì†Œì í•©](https://wikidocs.net/images/page/164364/05_under_good_overfit.png)

<br>
<br>
<br>

## ì •ê·œí™”
íŒŒë¼ë¯¸í„°ë“¤ì˜ í¬ê¸°ë¥¼ ì œí•œí•´ì„œ ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•´ì§€ëŠ”ê±¸ ë§‰ëŠ” ë°©ë²•<br>ì¦‰, <span style="background-color: #fff3cd"> ëª¨ë¸ì˜ ë³µì¡ë„ì— ëŒ€í•œ íŒ¨ë„í‹° </span> <br>
<br>

### ğŸ¾ ì •ê·œí™” ëª©ì 
**1. ê³¼ì í•© (overfitting ë°©ì§€)** <br>
- ì°¨ìˆ˜ê°€ ë†’ê³  íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜(ê³ ì°¨í•­ì˜ ê³„ìˆ˜)ê°€ ì»¤ì ¸ì„œ ë°ì´í„°ì˜ <u> ë…¸ì´ì¦ˆê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ </u> <br>-> <span style="color: blue">ê³¼ì í•© ë°©ì§€ ìœ„í•´ì„œ í•„ìš”í•¨</span>

**2. ê°„ë‹¨í•œ ëª¨ë¸ ìœ ë„** <br>
- ë†’ì€ ì°¨ìˆ˜ì˜ ê³„ìˆ˜ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ì„œ ë³µì¡ë„ ë‚®ì¶”ê¸°

**3. ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ** <br>
- ìƒˆë¡œìš´ ë°ì´í„°ì— ë” ì˜ ì‘ë™í•  ìˆ˜ ìˆë„ë¡

> L1 L2ëŠ” ìˆ˜ì—…ì—ì„œ ë‹¤ë£¨ì§€ ì•Šì•˜ì§€ë§Œ ê·¸ëƒ¥ ê¶ê¸ˆí•´ì„œ ì°¾ì•„ë´¤ë˜ ê¸°ì–µì´ .. ~

> ### ğŸ¾ L1 ì •ê·œí™” (L1 norm)  
> - **Norm**  
>   - ë²¡í„°ì˜ í¬ê¸°, ë‘ ë²¡í„° ê°„ ê±°ë¦¬ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜  
>   
> - **L1 Norm**  
>   - ë§¨í—ˆíŠ¼ ê±°ë¦¬ $d = |a_1 - b_1| + |a_2 - b_2|$  
>   - $\|x\|_1 = \sum_{i=1}^{n} |x_i|$  
>   
> - **L1 ì†ì‹¤**  
>   - $L = \sum_{i=1}^{n} |y_i - f(x_i)|$  
>   
> - **L1 ì •ê·œí™”**  
>   - $Loss = ê¸°ì¡´ \ Loss + \lambda \sum_j |w_j|$  
>   - ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ  
>   - í¸ë¯¸ë¶„í•  ë•Œ sgn ì‚¬ìš©  
>     $$
>       sgn(w) = 
>     \begin{cases}
>     1 & \text{if } w > 0 \\\\
>     0 & \text{if } w = 0 \\\\
>     -1 & \text{if } w < 0
>     \end{cases}
>     $$  
>   - ì—…ë°ì´íŠ¸ ì‹ :  
>     $w \leftarrow w - \frac{\eta \lambda}{n} \cdot sgn(w) - \eta \cdot \frac{\partial C_0}{\partial w}$  

> ### ğŸ¾ L2 ì •ê·œí™” (L2 norm)  
> - **L2 Norm**  
>   - ìœ í´ë¦¬ë“œ ê±°ë¦¬ ($ = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2}$)  
>   - $\|x\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}$  
>   
> - **L2 ì†ì‹¤**  
>   - $L = \sum_{i=1}^{n} (y_i - f(x_i))^2$  
>   
> - **L2 ì •ê·œí™”**  
>   - $Loss = ê¸°ì¡´ \ Loss + \lambda \sum_j w_j^2$  
>   - ê°€ì¤‘ì¹˜ë¥¼ 0ì— ê°€ê¹ê²Œ ë§Œë“¦ (0ì€ ì•„ë‹˜)  
>   - í¸ë¯¸ë¶„í•  ë•Œ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œí•¨  
>     $w \leftarrow \left(1 - \frac{\eta \lambda}{n} \right) w - \eta \cdot \frac{\partial C_0}{\partial w}$  
>   
> ![ì •ê·œí™”](https://blog.kakaocdn.net/dna/bENf8W/btrDXowms1A/AAAAAAAAAAAAAAAAAAAAAJl5YWJie9Uii1mukWC5aL2nzryNeHbkm4Albk_y8D6o/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1756652399&allow_ip=&allow_referer=&signature=S6CN9MArrlY4L70yiT3TCc3CqVw%3D)  
> ì¶œì²˜ ã…£ https://seongyun-dev.tistory.com/52


### ğŸ¾ ì •ê·œí™”ëœ ì„ í˜•íšŒê·€
#### ê¸°ì¡´ ì„ í˜•íšŒê·€ì˜ ë¹„ìš©í•¨ìˆ˜ (MSE)
$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 $


#### ì •ê·œí™”ëœ ì„ í˜•íšŒê·€ì˜ ë¹„ìš©í•¨ìˆ˜
$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2$

- $\lambda$ëŠ” ì •ê·œí™” ê³„ìˆ˜ (hyperparameter)
  - $\lambda$ê°€ í´ìˆ˜ë¡ $\theta_j^2$ ì‘ê²Œ ë§Œë“œë ¤ëŠ” ê²ƒ
  - <span style="color: blue">ë§Œì¼ $\lambda$ê°€ ë„ˆë¬´ í¬ë©´? (ì˜ˆ : $\lambda = 10^10$) </span>
    - overfitting ì œê±° ì‹¤íŒ¨, underfitting ë°œìƒ? -> $\lambda$ê°€ ë„ˆë¬´ ì»¤ì„œ ëª¨ë“  $\theta_j$ê°€ ì‘ì•„ì§
    - ê²½ì‚¬í•˜ê°•ë²• ìˆ˜ë ´ X -> $\lambda$ê°€ ë„ˆë¬´ ì»¤ì„œ cost function ë„ˆë¬´ í‰í‰í•˜ê±°ë‚­...
    - ì„¸íƒ€ ê°’ë“¤ì„ 0ì— ë„ˆë¬´ ê°•ì œë¡œ ë¬¶ì—¬ë‘  -> í•™ìŠµ ê±°ì˜ ì•ˆë˜ëŠ” ëª¨ë¸ì´ ë˜ì–´ ì¢‹ì€ ì„±ëŠ¥ ëª» ëƒ„
- - <span style="color: blue">ë§Œì¼ $\lambda$ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´? (ì˜ˆ : $\lambda = 10^10$) </span>
  - overfitting ë¬¸ì œ í•´ê²° X
  - í›ˆë ¨ë°ì´í„°ì—ë§Œ ë§ì¶°ì§
- $\theta_0$ (bias term)ëŠ” ì •ê·œí™” ëŒ€ìƒì—ì„œ ì œì™¸


<br>

### ğŸ¾ ì •ê·œí™”ëœ ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent with Regularization)
Repeat { <br>
    $\theta_0 := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_0^{(i)}$<br>
    $\theta_j := \theta_j - \alpha \cdot \left[ \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)} + \frac{\lambda}{m} \theta_j \right]$<br>
}

<br>

### ğŸ¾ ì •ê·œí™”ëœ ì •ê·œë°©ì •ì‹ (Regularized Normal Equation) 
ì´ ë¶€ë¶„ì€ ìˆ˜ì—… ì•ˆ í•˜ê³  ë„˜ì–´ê°€ì„œ ì‚¬ì‹¤ ì˜ ëª°ë¼ìš¤..
$\theta = \left( X^T X + \lambda \tilde{I} \right)^{-1} X^T y$


### ğŸ¾ ì •ê·œí™”ëœ ë¡œì§€ìŠ¤í‹± íšŒê·€
#### ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
$h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}$

#### ì •ê·œí™”ëœ ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ë¹„ìš©í•¨ìˆ˜ 
$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$

- ë‘ ë²ˆì§¸ í•­ì´ ì •ê·œí™” í•­
- $\theta_0$ëŠ” ì •ê·œí™”ì—ì„œ ì œì™¸ë¨

#### ì •ê·œí™”ëœ ê²½ì‚¬í•˜ê°•ë²•
Repeat { <br>
    $\theta_0 := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}$<br>
    $\theta_j := \theta_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} + \frac{\lambda}{m} \theta_j \right]$<br>
}

<br>

    ê°œì¸ ê³µë¶€ ê¸°ë¡ìš© ë¸”ë¡œê·¸ì…ë‹ˆë‹¤.
    ì˜¤ë¥˜ë‚˜ í‹€ë¦° ë¶€ë¶„ì´ ìˆì„ ê²½ìš° ì–¸ì œë“ ì§€ ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì§€ì í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤! â˜º

[ë§¨ ìœ„ë¡œ ì´ë™í•˜ê¸°](#){: .btn .btn--primary }{: .align-right}