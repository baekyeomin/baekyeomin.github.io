---
title: "[ML] 6. 클러스터링 (Clustering, especially Kmeans) "
categories: [AI, ML]
tags:
  - 머신러닝
  - 비지도학습
  - 클러스터링
  - kmeans
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}

## 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)
### 🐾 지도학습 (Supervised Learning)
- 데이터의 속성(attribute)와 목표(class) 속성 간의 패턴 발견 방법
- 데이터에 라벨(정답)이 있음
- 예측을 목표로 데이터 간의 관계 학습
  
### 🐾 비지도학습 (Unsupervised Learning)
- 데이터에 목표 속성 (target attribute) X
- 데이터 기반으로 숨겨진 패턴이나 구조를 찾아내는게 목표 !
- 데이터에 라벨 (정답)이 없음
  
<br>
<br>
<br>

## 클러스터링 (Clustering)
데이터를 유사한 그룹으로 묶는 기술
- <span style="color: blue">비지도학습임</span> -> 라벨링되어있지 않음
- <span style="background-color: #fff3cd">데이터 내의 유사성을 기반으로 데이터를 그룹화함</span>

<br>
**클러스터링의 이유**
- 라벨링 is expensive
- 데이터의 구조에 대한 통찰을 얻기 위해
  
> **여기서 잠깐!**
> ### distance function : 클러스터링에서 유사성/비유사성을 측정하는 함수
> #### 클러스터 내 유사성 (Intra Cluster Similarity)
> - <span style="background-color: #fff3cd">최소화 (같은 클러스터 내 데이터 포인트 가깝게)</span>
> #### 클러스터 간 유사성 (Inter Cluster Similarity)
> - <span style="background-color: #fff3cd">최대화 (타 클러스터들과 멀게)</span><br>
>
> **거리 측정이 가지는 특징**<br>
> 1. 대칭성 (Symmertry) <br>
>   D(a,b) = D(b,a)
> 2. 자기 유사성의 일정성 (Constancy)<br>
>   D(a,a) = 0
> 3. 양의 성질 (Positivity)<br>
>   D(a,a) = 0일때 A=B
> 4. 삼각부등식<br>
>   D(a,b) <= D(a,c) + D(B,C)
><br>
> [거리 측정법]<br>
> - **Single linkage** (가장 가까운 점 기준)
> - **Complete linkage** (가장 먼 점 기준)
> - **Average linkage** (평균 거리 기준)

<br>
<br>
<br>

### 🐾 Clustering 알고리즘
#### 1. 분할 클러스터링 (Partitional clustering)
데이터를 여러 클러스터로 나누는 방법
- Kmeans clustering...
- 비계층적이기에 1개의 클러스터를 출력함
- <span style="color: blue">사용자가 클러스터 K를 지정해야함</span>
  
#####  K-means Clustering 알고리즘
- D = {x1,...,xn}이 있다면 각 데이터 포인트 xi는 실수값 공간에서의 벡터
- 주어진 데이터 k개의 cluster로 분할

<span style="color: brown">1. k 값 정하기</span><br>
<span style="color: brown">2. k 개의 cluster 중심 (centroid) 초기화 (무작위)</span><br>
<span style="color: brown">3. 각 데이터 객체를 가장 가까운 클러스터 중심에 할당</span><br>
<span style="color: brown">4. 클러스터 중심 재조정 (각 cluster에 속한 객체들의 평균 위치로)</span><br>
<span style="color: brown">5. 수렴 확인 (아니면 2로 가서 반복)</span>

   
**kmeans clustering 순서를 그림으로!** <br>
![kmeans 클러스터링 그림](/assets/images/kmeans.png)  

**[수렴/정지 기준]**
- 데이터 포인트의 클러스터 재배정이 없거나 최소화될 때
  - 더 이상 데이터포인트가 타 클러스터로 이동하지 않으면 수렴!
- 중심 변화가 없거나 미미할 때
- 제곱오차합 (SSE)의 감소가 미미함 = 더이상 개선할게 없다고 판단
  - SSE (Squared Error) 최소화해야됨
  - 수식 : $SSE = \sum_{j=1}^k \sum_{x_i \in C_j} dist(x_i, m_j)^2$
  - $C_k$: k번째 클러스터  
  - $m_j$: 클러스터의 중심 (centroid)
  
**장점**
- 시간복잡도 ($O(tkn)$, t = 반복수, k = cluster 수, n = 객체 수), 대규모 데이터셋에 대해 효율적으로 동작
  
**단점**
- 평균이 정의되어야지만 사용가능 -> 카테고리형 데이터에는 사용불가 (대신 K-mode)
- 노이즈와 이상처리가 어려움 (outlier 제거 및 랜덤샘플링(전체 중 일부 택) 사용해야됨)
- K의 값을 미리 정해야함
- 비볼록 모양의 clustering 발견 불가
- 초기 시드에 민감함

<u> 약점에도 불구하고 단순성, 효율성 때문에 사용함 </u> <br>
<u> 타 clustering 알고리즘에 특정 데이터에 더 적합할 수는 있겠지만, <br>전반적으로 kmeans 보다 더 나은 성능을 보인다는 확실한 증거는 없음 </u> <br>
-> <span style="color: blue">정답이 없기에 clustering 알고리즘 간 비교는 어려움 </span>

**[거리 공식들]**
1. Minkowski Distance (p-노름)
    $dist(x_i, x_j) = \left( \sum_{l=1}^d |x_{il} - x_{jl}|^p \right)^{1/p}$
    - $p=1$: 맨해튼 거리 (L1 norm)  
   - $p=2$: 유클리드 거리 (L2 norm)
2. Manhattan Distance (L1)
    $dist(x_i, x_j) = \sum_{l=1}^d |x_{il} - x_{jl}|$
3. Euclidean Distance (L2)
   $dist(x_i, x_j) = \sqrt{\sum_{l=1}^d (x_{il} - x_{jl})^2}$
4. Weighted Euclidean Distance
   $dist(x_i, x_j) = \sqrt{\sum_{l=1}^d w_l (x_{il} - x_{jl})^2}$


#### 2. 계층적 클러스터링 (Hierarchical clustering)
클러스터를 트리 구조로 형성하는 방법 -> 덴드로그램(dendrogram)으로 표현<br>
![linear regression graph](/assets/images/dendrogram.png)  

- 클러스터 개수 미리 지정할 필요 X
- 대규모 데이터셋에 대해서는 확장성이 안 좋음
  

<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}