---
title: "[ML] 3. ë‹¤ë³€ëŸ‰ ì„ í˜•íšŒê·€ ë° ë‹¤í•­íšŒê·€ (Linear Regression with Multiple Variables and Polynomial regression) "
categories: [AI, ML]
tags:
  - ë¨¸ì‹ ëŸ¬ë‹
  - ì§€ë„í•™ìŠµ
  - íšŒê·€
  - ê²½ì‚¬í•˜ê°•ë²•
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

êµ­ë¯¼ëŒ€ ê¹€ì¥í˜¸ êµìˆ˜ë‹˜ "ë¨¸ì‹ ëŸ¬ë‹ê¸°ì´ˆ" ê³¼ëª©(2í•™ë…„ 1í•™ê¸° ìˆ˜ê°•)ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.
{: .notice--info}


## ë‹¤ë³€ìˆ˜ ì„ í˜•íšŒê·€ (Linear Regression with Multiple Variables)

### ğŸ¾ ì •ì˜
<span style="background-color: #fff3cd">ì…ë ¥ featureê°€ ì—¬ëŸ¬ê°œì¼ ë•Œì˜ ì„ í˜•íšŒê·€</span>
- ì˜ˆ : ì§‘ í¬ê¸°ì— ë”°ë¥¸ ì§‘ê°’ ì˜ˆì¸¡ 

| Size (feetÂ²) | Bedrooms ($x_2$) | Floors ($x_3$) | Age ($x_4$) | Price ($y$) |
|--------------|-----------------|----------------|-------------|-------------|
| 2104         | 3               | 1              | 45          | 460         |
| 1416         | 3               | ?              | 40          | 232         |
| 1534         | 3               | 2              | 30          | 315         |
| ...          | ...             | ...            | ...         | ...         |


**ê° í–‰ = í•˜ë‚˜ì˜ í•™ìŠµ ìƒ˜í”Œ

<br>

### ğŸ¾ ê°€ì„¤í•¨ìˆ˜
- ë‹¨ì¼ë³€ìˆ˜ ì„ í˜•íšŒê·€ : <span style="font-size:150%"> $ h_\theta(x) = \theta_0 + \theta_1 x $ </span>
- ë‹¤ë³€ìˆ˜ ì„ í˜•íšŒê·€ : <br>
  íŠ¹ì§•ì´ nê°œ ì¼ ë•Œ.. <span style="font-size:150%">$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n = \theta^T x $</span>

  ì´ë¥¼ ë²¡í„°í˜•íƒœë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 
  (ì‚¬ì§„ë„ ì°¸ê³ í•˜ì„¸ìš¤)
  ![linear regression graph](/assets/images/vector.png)  
span style="font-size:150%"> $x = [x_0, x_1, ..., x_n]^T \in \mathbb{R}^{n+1}$  (ë‹¨, $x_0 = 1$)  </span>
span style="font-size:150%"> $\theta = [\theta_0, \theta_1, ..., \theta_n]^T \in \mathbb{R}^{n+1}$ </span> <br>
 ì…ë ¥ xì™€ íŒŒë¼ë¯¸í„° ì„¸íƒ€ëŠ” n+1 ì°¨ì› ë²¡í„°

<br>

### ğŸ¾ ë¹„ìš© í•¨ìˆ˜ (Cost Function)
span style="font-size:150%"> $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)^2$ </span>

<span style="background-color: #fff3cd">cost ìµœì†Œí™” í•˜ëŠ” ìµœì ì˜ $\theta$ ì°¾ê¸° ! </span>

<br>
<br>
<br>

## ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent)
ë‹¨ì¼ ë³€ìˆ˜ ì„ í˜• íšŒê·€ì˜ ê²½ì‚¬í•˜ê°•ë²• (n=1) : <br>
Repeat{<br>
    $\theta_0 := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)$ <br>
    $\theta_1 := \theta_1 - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big) x^{(i)}$<br>
}
<br>
ë‹¤ë³€ìˆ˜ ì„ í˜• íšŒê·€ì˜ ê²½ì‚¬í•˜ê°•ë²• (n>=1) :<br>
Repeat{<br>
    $\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big) x_j^{(i)}$<br>
}<br>
<br>
- $\alpha$: í•™ìŠµë¥  (learning rate)  
- $m$: í•™ìŠµ ìƒ˜í”Œ ìˆ˜  
- $h_\theta(x^{(i)})$: ië²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•œ ì˜ˆì¸¡ê°’  
- $y^{(i)}$: ì‹¤ì œê°’  

<br> 
<br> 
<br> 

### ğŸ¾ Feature Scaling
ê²½ì‚¬í•˜ê°•ë²•ì„ ì ìš©í•  ë•Œ, ê° featureë“¤ì˜ ê°’ ë²”ìœ„ê°€ í¬ê²Œ ì°¨ì´ ë‚˜ë©´ í•™ìŠµì´ ì˜ ì•ˆ ë  ìˆ˜ ìˆë‹¤.<br>
ê·¸ë˜ì„œ <span style="background-color: #fff3cd">ëª¨ë“  ì…ë ¥ë³€ìˆ˜ (feature)ë“¤ì´ ë¹„ìŠ·í•œ í¬ê¸°(scale)ì„ ê°–ë„ë¡ ë§Œë“œëŠ” ê²ƒ</span>ì´ Feature Scaling!
<br>
<br>
- **íŠ¹ì§•ë“¤ì˜ í¬ê¸° ë²”ìœ„ë¥¼ ë§ì¶°ì£¼ë©´ ìˆ˜ë ´ ì†ë„ê°€ ì¦ê°€**
- contour plot (ë¹„ìš©í•¨ìˆ˜ Jì˜ ë“±ê³ ì„ )ì´ ì›í˜•/íƒ€ì›í˜•ìœ¼ë¡œ ì´ì˜ê²Œ ë‚˜ì˜¤ë„ë¡ ë³€í™”í•´ì£¼ê¸°!

> **Feature Scaling ë°©ë²•ë“¤**<br>
> <span style="color: blue">**1. Min-Max ì •ê·œí™”**</span> <br>
> ê°ê°ì„ ìµœëŒ€ê°’ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ [0,1] ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ëŠ” ë°©ë²• <br>
> ìˆ˜ì‹ :  <span style="font-size:150%">$x_j' = \frac{x_j - \min(x_j)}{\max(x_j) - \min(x_j)}$ </span>
> - $\min(x_j)$ : jë²ˆì§¸ íŠ¹ì§•ì˜ ìµœì†Œê°’  
> - $\max(x_j)$ : jë²ˆì§¸ íŠ¹ì§•ì˜ ìµœëŒ€ê°’  
> <br>
> <br>
> <span style="color: blue">**2. Mean Normalization (í‰ê· ì •ê·œí™”)**</span> <br>
> íŠ¹ì§•(feature)ì˜ í‰ê· ì„ 0ì— ê°€ê¹ê²Œ í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë²”ìœ„ë¥¼ [-1,1] ì •ë„ë¡œ ë§ì¶”ëŠ” ë°©ë²• <br>
> ìˆ˜ì‹ : <span style="font-size:150%">$x_j' = \frac{x_j - \mu_j}{\max(x_j) - \min(x_j)}$</span>
> - $\mu_j$: jë²ˆì§¸ íŠ¹ì§•ì˜ í‰ê· ê°’  
> - $x_0 = 1$ (bias í•­)ì€ ì •ê·œí™” ì ìš©í•˜ì§€ ì•ŠìŒ! <br> <span style="color: red">bias termì€ ê³ ì •ëœ ìƒìˆ˜ì„! </span>



<br>

### ğŸ¾ Learning Rate (í•™ìŠµë¥ )
<span style="background-color: #fff3cd">$J(\theta)$ëŠ” ë§¤ë°˜ë³µë§ˆë‹¤ ê°ì†Œí•´ì•¼í•¨.</span> <br>
- ìˆ˜ë ´ íŒë‹¨ ì¡°ê±´ ì˜ˆì‹œ:<br>
span style="font-size:150%"> $J(\theta^{(k+1)}) - J(\theta^{(k)}) < 10^{-3}$ </span> <br>
(â†’ ë” ì´ìƒ ê°ì†Œê°€ í¬ì§€ ì•Šìœ¼ë©´ ìˆ˜ë ´í–ˆë‹¤ê³  íŒë‹¨)

<span style="color: red"> **ë°˜ë³µ íšŸìˆ˜ë³´ë‹¤ ì˜ ì¤„ì–´ë“¤ê³  ìˆëŠ”ê°€ê°€ ë” ì¤‘ìš”í•¨!!** </span> <br>
<br>

> **1. í•™ìŠµë¥ ì´ ë„ˆë¬´ í° ê²½ìš°**<br>
> - <span style="color: blue"> ë°œì‚° (divergence)ê°€ëŠ¥</span>
> - $J(\theta)$ ê°’ì´ ì¤„ì–´ë“¤ì§€ ì•Šê³  ì˜¤íˆë ¤ ì»¤ì§€ê±°ë‚˜ ì§„ë™í•  ìˆ˜ë„ ìˆìŒ
> - í•™ìŠµ ì‹¤íŒ¨ ê°€ëŠ¥
>
> <br>
>
> **2. í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì€ ê²½ìš°** <br>
> - $J(\theta)$ê°€ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œí•˜ê¸´ í•˜ì§€ë§Œ<u>**ë§¤ìš° ëŠë¦¬ê²Œ ìˆ˜ë ´** </u>
> - ê³„ì‚° ì†ë„ ëŠë¦¼

ì—¬ëŸ¬ ê°’ì„ ì§ì ‘ ì‹œë„í•´ë³´ë©° íƒìƒ‰ì´ í•„ìš”í•˜ë‹¤ 

<br>
<br>

## ë‹¤í•­íšŒê·€ (Polynomial Regression)
íŠ¹ì§•(feature)ì˜ ì°¨ìˆ˜ë¥¼ ëŠ˜ë ¤, <span style="color: blue">ë¹„ì„ í˜• ë°ì´í„°ë¥¼ í‘œí˜„</span> <br>
<br>
ì˜ˆ : $ h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$

- ì ì ˆí•œ feature ì„ íƒìœ¼ë¡œ overfitting (ê³¼ì í•©) ë° underfitting(ê³¼ì†Œì í•©)ì´ ë˜ì§€ ì•Šë„ë¡í•˜ì!

<br>
<br>
<br>

## Normal Equation (ì •ê·œë°©ì •ì‹) -> ìˆ˜ì¹˜í•´ì„ í•„ê¸° ì°¸ê³ í•˜ê¸°
<span style="background-color: #fff3cd"> ë°˜ë³µ ì—†ì´, ìˆ˜í•™ì ìœ¼ë¡œ ë¹„ìš©í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ì—¬ ì§ì ‘ í•´ë¥¼ êµ¬í•˜ëŠ” ë°©ë²• </span><br>
<br>
- ì¥ì 
  - ë°˜ë³µ í•„ìš” ì—†ìŒ (ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë‹¬ë¦¬)  
  - feature scaling í•„ìš” ì—†ìŒ  
  - í•™ìŠµë¥  ì„ íƒí•  í•„ìš” ì—†ìŒ
- ë‹¨ì 
  - ì—­í–‰ë ¬ ê³„ì‚°í•´ì•¼í•´ì„œ ì‹œê°„ë³µì¡ë„ ë†’ì„ ìˆ˜ë„, ê³„ì‚°ì†ë„ ëŠë ¤ì§ˆ ìˆ˜ë„
<br>
<br>

### ğŸ¾ ì—­í–‰ë ¬ ë¶ˆê°€ì¸ ê²½ìš° (ì„ í˜•ëŒ€ìˆ˜ ë‚´ìš©)
1. featureê°€ ë„ˆë¬´ ë§ì„ ë•Œ
    - ë¶ˆí•„ìš”í•œ feature ì‚­ì œ  
2. feature ê°„ì˜ ì¤‘ë³µëœ ì •ë³´ê°€ ìˆì„ ë•Œ (=ì„ í˜•ì¢…ì†)
    - ì¤‘ë³µ ì œê±°
  
<br>

    ê°œì¸ ê³µë¶€ ê¸°ë¡ìš© ë¸”ë¡œê·¸ì…ë‹ˆë‹¤.
    ì˜¤ë¥˜ë‚˜ í‹€ë¦° ë¶€ë¶„ì´ ìˆì„ ê²½ìš° ì–¸ì œë“ ì§€ ê¸€ì´ë‚˜ ë©”ì¼ë¡œ ì§€ì í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤! â˜º

[ë§¨ ìœ„ë¡œ ì´ë™í•˜ê¸°](#){: .btn .btn--primary }{: .align-right}