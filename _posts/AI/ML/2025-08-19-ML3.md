---
title: "[ML] 3. 다변량 선형회귀 및 다항회귀 (Linear Regression with Multiple Variables and Polynomial regression) "
categories: [AI, ML]
tags:
  - 머신러닝
  - 지도학습
  - 회귀
  - 경사하강법
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}


## 다변수 선형회귀 (Linear Regression with Multiple Variables)

### 🐾 정의
<span style="background-color: #fff3cd">입력 feature가 여러개일 때의 선형회귀</span>
- 예 : 집 크기에 따른 집값 예측 

| Size (feet²) | Bedrooms ($x_2$) | Floors ($x_3$) | Age ($x_4$) | Price ($y$) |
|--------------|-----------------|----------------|-------------|-------------|
| 2104         | 3               | 1              | 45          | 460         |
| 1416         | 3               | ?              | 40          | 232         |
| 1534         | 3               | 2              | 30          | 315         |
| ...          | ...             | ...            | ...         | ...         |


**각 행 = 하나의 학습 샘플

<br>

### 🐾 가설함수
- 단일변수 선형회귀 : <span style="font-size:150%"> $ h_\theta(x) = \theta_0 + \theta_1 x $ </span>
- 다변수 선형회귀 : <br>
  특징이 n개 일 때.. <span style="font-size:150%">$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n = \theta^T x $</span>

  이를 벡터형태로 표현하면 다음과 같다. 
  (사진도 참고하세욤)
  ![linear regression graph](/assets/images/vector.png)  
span style="font-size:150%"> $x = [x_0, x_1, ..., x_n]^T \in \mathbb{R}^{n+1}$  (단, $x_0 = 1$)  </span>
span style="font-size:150%"> $\theta = [\theta_0, \theta_1, ..., \theta_n]^T \in \mathbb{R}^{n+1}$ </span> <br>
 입력 x와 파라미터 세타는 n+1 차원 벡터

<br>

### 🐾 비용 함수 (Cost Function)
span style="font-size:150%"> $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)^2$ </span>

<span style="background-color: #fff3cd">cost 최소화 하는 최적의 $\theta$ 찾기 ! </span>

<br>
<br>
<br>

## 경사하강법 (Gradient Descent)
단일 변수 선형 회귀의 경사하강법 (n=1) : <br>
Repeat{<br>
    $\theta_0 := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big)$ <br>
    $\theta_1 := \theta_1 - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big) x^{(i)}$<br>
}
<br>
다변수 선형 회귀의 경사하강법 (n>=1) :<br>
Repeat{<br>
    $\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)}\Big) x_j^{(i)}$<br>
}<br>
<br>
- $\alpha$: 학습률 (learning rate)  
- $m$: 학습 샘플 수  
- $h_\theta(x^{(i)})$: i번째 샘플에 대한 예측값  
- $y^{(i)}$: 실제값  

<br> 
<br> 
<br> 

### 🐾 Feature Scaling
경사하강법을 적용할 때, 각 feature들의 값 범위가 크게 차이 나면 학습이 잘 안 될 수 있다.<br>
그래서 <span style="background-color: #fff3cd">모든 입력변수 (feature)들이 비슷한 크기(scale)을 갖도록 만드는 것</span>이 Feature Scaling!
<br>
<br>
- **특징들의 크기 범위를 맞춰주면 수렴 속도가 증가**
- contour plot (비용함수 J의 등고선)이 원형/타원형으로 이쁘게 나오도록 변화해주기!

> **Feature Scaling 방법들**<br>
> <span style="color: blue">**1. Min-Max 정규화**</span> <br>
> 각각을 최대값으로 나누어서 [0,1] 범위로 정규화하는 방법 <br>
> 수식 :  <span style="font-size:150%">$x_j' = \frac{x_j - \min(x_j)}{\max(x_j) - \min(x_j)}$ </span>
> - $\min(x_j)$ : j번째 특징의 최소값  
> - $\max(x_j)$ : j번째 특징의 최대값  
> <br>
> <br>
> <span style="color: blue">**2. Mean Normalization (평균정규화)**</span> <br>
> 특징(feature)의 평균을 0에 가깝게 하는 방법으로, 범위를 [-1,1] 정도로 맞추는 방법 <br>
> 수식 : <span style="font-size:150%">$x_j' = \frac{x_j - \mu_j}{\max(x_j) - \min(x_j)}$</span>
> - $\mu_j$: j번째 특징의 평균값  
> - $x_0 = 1$ (bias 항)은 정규화 적용하지 않음! <br> <span style="color: red">bias term은 고정된 상수임! </span>



<br>

### 🐾 Learning Rate (학습률)
<span style="background-color: #fff3cd">$J(\theta)$는 매반복마다 감소해야함.</span> <br>
- 수렴 판단 조건 예시:<br>
span style="font-size:150%"> $J(\theta^{(k+1)}) - J(\theta^{(k)}) < 10^{-3}$ </span> <br>
(→ 더 이상 감소가 크지 않으면 수렴했다고 판단)

<span style="color: red"> **반복 횟수보다 잘 줄어들고 있는가가 더 중요함!!** </span> <br>
<br>

> **1. 학습률이 너무 큰 경우**<br>
> - <span style="color: blue"> 발산 (divergence)가능</span>
> - $J(\theta)$ 값이 줄어들지 않고 오히려 커지거나 진동할 수도 있음
> - 학습 실패 가능
>
> <br>
>
> **2. 학습률이 너무 작은 경우** <br>
> - $J(\theta)$가 안정적으로 감소하긴 하지만<u>**매우 느리게 수렴** </u>
> - 계산 속도 느림

여러 값을 직접 시도해보며 탐색이 필요하다 

<br>
<br>

## 다항회귀 (Polynomial Regression)
특징(feature)의 차수를 늘려, <span style="color: blue">비선형 데이터를 표현</span> <br>
<br>
예 : $ h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$

- 적절한 feature 선택으로 overfitting (과적합) 및 underfitting(과소적합)이 되지 않도록하자!

<br>
<br>
<br>

## Normal Equation (정규방정식) -> 수치해석 필기 참고하기
<span style="background-color: #fff3cd"> 반복 없이, 수학적으로 비용함수를 미분하여 직접 해를 구하는 방법 </span><br>
<br>
- 장점
  - 반복 필요 없음 (경사하강법과 달리)  
  - feature scaling 필요 없음  
  - 학습률 선택할 필요 없음
- 단점
  - 역행렬 계산해야해서 시간복잡도 높을 수도, 계산속도 느려질 수도
<br>
<br>

### 🐾 역행렬 불가인 경우 (선형대수 내용)
1. feature가 너무 많을 때
    - 불필요한 feature 삭제  
2. feature 간의 중복된 정보가 있을 때 (=선형종속)
    - 중복 제거
  
<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}