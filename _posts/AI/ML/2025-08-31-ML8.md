---
title: "[ML] 8. Bagging & Boosting "
categories: [AI, ML]
tags:
  - 머신러닝
  - 지도학습
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}

## 앙상블 기법
여러 개의 모델을 결합해서 개별 모델보다 더 나은 성능을 내게 하는 법
ex. KNN, 선형 모델, 신경망, 결정 트리.. 등과 같은 개별 모델들을 조합해서 성능을 향상시키기

<br>

## Bias / Variance Decomposition
<span style="font-size:200%">  $E[(y-\hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \sigma^2$ </span>
<br>
어떤 입력 x에 대해 모델이 내리는 예측 y는 random variable처럼 생각하쟈 <br>
-> 학습 데이터셋이 달라질 때마다 모델이 달라질 수 있기 때문<br>

**Bias**
- 예측 평균이 진짜 정답과 얼마나 떨어져있는가 (예측 평균 ~ 실제 값 거리)
- <span style="color: blue">Bias가 크면 -> underfitting </span>
<br>

**Variance**
- 데이터셋이 바뀔 때 예측값이 얼마나 달라지는가(예측  분포의 퍼짐)
- <span style="color: blue">Variance가 크면 -> overfitting </span>
<br>

**베이즈오차**
- 출력 자체에 내재된 오차
- 예측 불가하고 어떤 모델도 줄일 수 X

<br>

**Bias / Variance Decomposition 시각화**
![시각화1](/assets/images/bias_variance_error1.png)  
![시각화2](/assets/images/bias_variance_error2.png)  

<br>
<br>
<br>

## Bagging (Bootstrap Aggregating)
데이터 샘플링을 통해 모델의 <span style="color: blue"> 분산을 줄이고 </span> 일반화 성능을 높이는 방법

<br>
<span style="background-color: #fff3cd">주어진 학습 데이터셋 D에서 샘플 복원 추출하여 여러 개의 가데이터셋 생성하는 것 </span> <br>
<br>
- 실수값 예측인 경우 각 모델이 확률 형태로 예측 반환하고, 이진 분류일 경우 0또는1로 예측
- - 여러 모델의 예츨을 평균내서 최종 예측 결정
        $y_{\text{bagged}} = I\!\left(\frac{1}{m}\sum_{i=1}^m p_i > 0.5\right)$

1. n개의 example 있는 하나의 데이터셋 D
2. m개의 새로운 데이터셋 (bootstrap sample) 생성
3. 각 bootstrap sample로 모델 학습 -> 각 모델의 예측값을 평균

- **장점** : 분산 감소 -> 과적합 위험 감소, 일반화 성능향상
- **단점** : Bias(편향) 은 줄여주지 못함

![배깅예시](/assets/images/bagging_ex.png)
위 사진은 복원 추출로 m개의 n=7인 bootstrap sample을 생성한 그림이다.<br> 각각의 샘플을 모델학습한 후 예측을 했을 때 모델의 분산이 약 1/m배로 줄어든다

### 🐾 Bagging 에서 정말 분산이 1/m 감소하나?
모델들이 **독립**이라고 가정하면 : <br>
$Var\!\left(\frac{1}{m}\sum_{i=1}^m y_i \right) = \frac{1}{m}\sigma^2$
<br> <br>
하지만 실제로는 예측값들 사이의 **상관관계** $\rho$ 가 존재: <br>
$Var\!\left(\frac{1}{m}\sum_{i=1}^m y_i \right) 
= \frac{1}{m}(1-\rho)\sigma^2 + \rho\sigma^2$
<br><br>
- <span style="color: blue">상관관계 (피어슨 상관계수 : $\rho = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$) 높을수록 분산 감소 효과는 낮다</span>
- - $\rho = 1$: 완벽한 양의 상관관계 (같이 증가/감소)
- $\rho = -1$: 완벽한 음의 상관관계 (반대로 움직임)
- $\rho = 0$: 상관 없음 (독립적이라고 볼 수 있음)

<br> <br> <br>

### 🐾 Random Forest
<span style="background-color: #fff3cd">Bagged Decision Tree + 무작위 특성 선택(feature selection) </span>
<br><br>
- 여러 개의 결정트리 학습시켜서 예측값 앙상블 (평균 or 다수결)한 기법
- Bagging 이용해서 각 트리마다 bootstrap sample 사용해서 학습함 (데이터 다양성 확보)
- Random Forest는 각 트리의 노드를 나눌 때 전체 특성 중 random 선택된 d개의 특성을 고려한다. 
- 따라서 트리 간 예측이 비슷하지 않고, 예측값들 간 상관성이 낮아서 Bagging 분산 감소 효과가 커진다. 

<br> 

### 🐾 정리
- bias는 그대로, variance만 감소
- classifier 사이에 상관관계 존재 O -> 랜덤포레스트로 무작위성 높일 수 있음
- 모든 구성은 동일한 가중치로 평균 (=단순결합 naive mixture)

<br> <br> <br>

## Boosting
분류기를 순차적으로 학습 ,<br>-> 이전 모델이 틀린 예제에 더 집중해서 다음 모델을 학습하도록함
<br>
- 일반적인 오분류율 (misclassification rate) : 
  - $\frac{1}{N} \sum I(h(x_i) \neq y_i)$
- 새로운 아이디어 : 
  - $\frac{1}{N} \sum w_i I(h(x_i) \neq y_i)$
  - 가중치$w_i$를 두어, 틀린 샘플에 집중하도록 학습 (단, 가중치 총합은 1이 되어야함)
<br>


<span style="background-color: #fff3cd"><u> 특징 </u> </span> <br>
- 반복학습법, 약한 분류기(weak learner)를 강하게 만들기.
- **장점**:
  - 단순한 weak learner (예: Decision stump)만으로도 성능 향상.
  - bias 감소 → underfitting 방지.
- **단점**:
  - 노이즈에 민감.
  - 틀린 데이터에 집중하다 보니 **overfitting 위험** 존재

### 🐾 AdaBoost (Adaptive Boosting)
기본 분류기가 주어졌을 때...
1. 각 반복마다 <span style="background-color: #fff3cd">이전에 틀린 샘플에 더 큰 가중치를 주어</span> 학습 데이터 샘플의 가중치를 재조정
2. 재조정된 가중치 기반으로 새로운 기본 분류기 학습
3. 오분류율에 따라 가중치 업데이트
4. 반복
<br>




<span style="background-color: #fff3cd"><u> 요구조건 </u> </span> <br>
- 가중치가 부여된 오류를 최소화할 수 있어야함
- 앙상블 크기가 커질 수 있기에 <span style="color = red"> 기본 분류기는 빠르게 학습 가능한 모델이어야함</span>
  - 따라서 Weak Learner / Classifier만으로도 충분함
<br> <br>
더 상세하게..
Input : 데이터셋 $D = \{(x^1, t^1), \dots, (x^N, t^N)\}$
<br>
과정 : <br>
1. 초기화: 모든 샘플에 같은 가중치 부여  
   $w_n^1 = \frac{1}{N}$

2. $t$번째 단계에서, weak classifier $h_t(x)$ 학습:  
   $h_t = \arg\min_h \sum_{n=1}^N w_n^t I(h(x^n) \neq t^n)$

3. 가중오차 계산:  
   $err_t = \sum_n w_n^t I(h_t(x^n) \neq t^n)$

4. 가중치 업데이트를 위한 신뢰도 계산:  
   $\alpha_t = \frac{1}{2} \log \left(\frac{1 - err_t}{err_t}\right)$

5. 가중치 업데이트:  
   $w_n^{t+1} = w_n^t \cdot \exp(\alpha_t I(h_t(x^n) \neq t^n))$

6. 최종 앙상블 분류기:  
   $H(x) = \text{sign}\left(\sum_t \alpha_t h_t(x)\right)$


### 🐾 Weak Learner / Classifier
- <span style="background-color: #fff3cd">계산적으로 효율적</span>이어야함
- <span style="background-color: #fff3cd">빠르게 학습</span>이 가능해야함
- 예: Decision tree의 약한 버전 (decision stump: 한 번의 split만 있는 트리)
<span style="color = blue"> 무작위 추축 (50%)보다 조금이라도 나은 성능을 보인다면, 이 classifier를 ADABOOST로 반복 결합하면 강한 모델 만들 수 O</span>

##  Boosting vs Bagging 

| 구분 | Boosting | Bagging |
|------|----------|---------|
| **목적** | Bias 감소 | Variance 감소 |
| **Bias** | ↓ (Underfitting 방지) | 크게 변하지 않음 |
| **Variance** | ↑ (증가 가능) | ↓ (감소, Overfitting 방지) |
| **구조** | 순차적 | 병렬적 |
| **의존성** | 높음 (앞 단계 결과에 의존) | 낮음 (독립적으로 학습) |
| **다양성 확보** | 동일 데이터에 반복 → error 집중 | Bootstrap 샘플링 → 데이터 다양성 |
| **예시** | AdaBoost | Random Forest, Bagged Trees |

<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}