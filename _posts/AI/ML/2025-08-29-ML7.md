---
title: "[ML] 7. SVM (Support Vector Machine) "
categories: [AI, ML]
tags:
  - 머신러닝
  - 지도학습
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}

## SVM
- <span style="color: blue">지도학습</span> 알고리즘 중 하나
- <span style="color: blue">분류(classification)</span>에 주로 사용
- 주어진 데이터 포인트를 가능한 한 잘 구분할 수 있는 <span style="background-color: #fff3cd">decision boundary (= 두 class를 분리하는 선이나 초평면)을 찾는 것이 핵심 아이디어 !</span>
  - 최대 마진을 가지도록하기 
    - 마진 : decision boundary와 가장 가까운 데이터 포인트 (=support vector) 사이 거리
    - <span style="background-color: #fff3cd">마진이 클수록 새 데이터 포인트에 대해 더 잘 일반화 될 가능성이 크다!</span>
- 다중 분류는 `one-vs-all` 또는 `one-vs-one` 전략으로 확장

<br>
<br>
<br>

## 로지스틱 회귀와 비교
### 🐾 [로지스틱 회귀] 손실함수
<u> 이진 크로스 엔트로피 </u> <br>
$J(\theta) = -\frac{1}{m}\sum_{i=1}^m \Big[y^{(i)}\log h_\theta(x^{(i)}) + \big(1-y^{(i)}\big)\log \big(1-h_\theta(x^{(i)})\big)\Big]$
- y=1일 때 : $z=\theta^Tx$가 **클수록** 비용 ↓
- y=0일 때 : $z=\theta^Tx$가 **작을수록** 비용 ↓

### 🐾 [SVM] 손실함수
![kmeans 클러스터링 그림](/assets/images/hingeloss.png)  
<u> Hinge Loss </u> <br>
$\ell\big(y, f(x)\big)=\max\big(0,\, 1 - y\cdot f(x)\big),\quad f(x)=\theta^T x + b$
- SVM은 -1,+1로 하기에 부호가 일치하면 손실 = 0
- 마진은 손실의 크기가 클수록 작다. 
  - 마진이 크다는 것은 데이터가 결정 경계에서 멀리 떨어져 있다는 뜻이기에 분류가 확실하게 잘 되었다는 뜻이기 때문 !
  
<u> 손실함수 </u> <br>
$\min_{\theta,b}\; \frac{1}{2}\|\theta\|^2 \;+\; C\sum_{i=1}^m \max\!\big(0,\,1-y_i(\theta^T x_i+b)\big)$
