---
title: "[ML] 7. SVM (Support Vector Machine) "
categories: [AI, ML]
tags:
  - 머신러닝
  - 지도학습
layout: single
toc : true
toc_sticky: true
comments: true
use_math: true
---

국민대 김장호 교수님 "머신러닝기초" 과목(2학년 1학기 수강)을 바탕으로 학습한 내용을 정리한 글입니다.
{: .notice--info}

## SVM
- <span style="color: blue">지도학습</span> 알고리즘 중 하나
- <span style="color: blue">분류(classification)</span>에 주로 사용
- 주어진 데이터 포인트를 가능한 한 잘 구분할 수 있는 <span style="background-color: #fff3cd">decision boundary (= 두 class를 분리하는 선이나 초평면)을 찾는 것이 핵심 아이디어 !</span>
  - 최대 마진을 가지도록하기 
    - 마진 : decision boundary와 가장 가까운 데이터 포인트 (=support vector) 사이 거리
    - <span style="background-color: #fff3cd">마진이 클수록 새 데이터 포인트에 대해 더 잘 일반화 될 가능성이 크다!</span>
- 다중 분류는 `one-vs-all` 또는 `one-vs-one` 전략으로 확장
<br>

> ## 결정 경계와 마진 (선형 분리 가정)
> - **결정 경계(Decision Boundary)**:  
>   $\theta^T x + b = 0$
> 
> - **두 마진 경계**:  
>   $\theta^T x + b = \pm 1$
> 
> - **마진 폭 (Margin Width)**:  
>   $\text{margin width} = \frac{2}{\|\theta\|}$
> - **서포트 벡터(Support Vector)**:  
>   마진 경계(=\(\pm1\))에 **닿아 있는 점들**.  

<br>
<br>
<br>

## 로지스틱 회귀와 비교
### 🐾 [로지스틱 회귀] 손실함수
<br>
<span style="background-color: #fff3cd"><u> 이진 크로스 엔트로피 </u> </span> <br>
$J(\theta) = -\frac{1}{m}\sum_{i=1}^m \Big[y^{(i)}\log h_\theta(x^{(i)}) + \big(1-y^{(i)}\big)\log \big(1-h_\theta(x^{(i)})\big)\Big]$
- y=1일 때 : $z=\theta^Tx$가 **클수록** 손실 ↓
- y=0일 때 : $z=\theta^Tx$가 **작을수록** 손실 ↓

### 🐾 [SVM] 손실함수
![kmeans 클러스터링 그림](/assets/images/hingeloss.png)  
<br>
<span style="background-color: #fff3cd"> <u> Hinge Loss </u>  </span> <br>
$\ell\big(y, f(x)\big)=\max\big(0,\, 1 - y\cdot f(x)\big),\quad f(x)=\theta^T x + b$
<br>
여기서 $y \in \{-1, +1\}$ 이기에 부호가 일치하면 손실 = 0
- 마진은 손실의 크기가 클수록 작다. 
  - 마진이 크다는 것은 데이터가 결정 경계에서 멀리 떨어져 있다는 뜻이기에 분류가 확실하게 잘 되었다는 뜻이기 때문 !
  
<span style="background-color: #fff3cd"><u> 손실함수 </u> </span> <br>
$\min_{\theta,b}\; \frac{1}{2}\|\theta\|^2 \;+\; C\sum_{i=1}^m \max\big(0,\,1-y_i(\theta^T x_i+b)\big)$
<br>
- 첫 번째 항:  
  $\tfrac{1}{2}\|\theta\|^2$ → **마진 최대화** 효과 (가중치를 작게 유지).
- 두 번째 항:  
  $C\sum \max(0,1-y_i(\theta^T x_i+b))$ → **힌지 로스 합** (오분류/작은 마진에 대한 페널티).
- C: 오분류를 얼마나 허용할지 조절하는 하이퍼파라미터
  - C값 크면 → 오분류를 거의 허용 안 함 (분산↑, 과적합 위험).
  - C값 작으면 → 오분류를 허용 (편향↑, 과소적합 위험).


> **힌지 로스**: 데이터 한 점에 대한 손실
> **SVM 손실함수**:  
>  - **정규화 항 (마진 관련)** + **힌지 로스 합**  
>   - 즉, **힌지 로스를 최소화하면서 마진은 크게 하는 것이 핵심**
>
> $\min_{\theta,b}\; \frac{1}{2}\|\theta\|^2 + C \sum \text{힌지로스}$

<br>
<br>
<br>

## 수학적으로 본 SVM 
마진을 구할 때 벡터의 내적이나 정사영의 개념을 알아두어야한다. 
해당 내용은 아래 노트필기 사진 참고하기 !
![svm_math](/assets/images/svm_math.png)  

<br>
<br>
<br>

### SVM 커널 트릭
### 🐾 커널트릭 필요 이유
단순한 선형 결정 경계로는 분류 불가한 경우, <br>
비선형 데이터를 선형적으로 구분할 수 있게 특성변환이 필요하다!

### 🐾 커널 트릭 (Kernel Trick)
x에 대해 랜드마크와의 유사도 기반으로 새로운 특징벡터를 계산하는 과정

1. 주어진 데이터가 있음
2. 랜드마크를 지정함
3. 유사도를 계산함
4. 훈련데이터에 대해 유사도 벡터 계산
$f(x) = [K(x,l^{(1)}), K(x,l^{(2)}), \dots, K(x,l^{(m)})]$ <br>
-> 이 과정을 통해 데이터 차원 확장, $x^i$는 고차원 공간으로 매핑

- 직접 고차원으로 매핑하지 않고, **내적**을 통해 계산
- $K(x,z)=\phi(x)\cdot\phi(z)$
- 다양한 커널(선형 커널, string 커널, chi-square 커널.. 등) 이 있지만, 머신러닝기초 과목에서는 가우시안 커널만 짚고 넘어감
  
<span style="background-color: #fff3cd"><u> 가우시안 커널 </u> </span> <br>

$K(x,z)=\exp\left(-\frac{\|x-z\|^2}{2\sigma^2}\right)$

- 두 벡터의 거리가 가까우면 값이 1, 멀어지면 0에 가까워짐
- $f_i(x)=\exp\left(-\frac{(x-l^{(i)})^2}{2\sigma^2}\right)$ 에서 $l^i$는 기준점(랜드마크), x는 입력데이터
- $\sigma^2$ (분산) 이 작으면 sharp peak, 크면 넓게 퍼짐
- 랜드마크 선택은
  - 랜덤 선택
  - 데이터 분포 기반 선택 ...


<span style="background-color: #fff3cd"><u> $C$와 $\sigma$의 영향 </u> </span> <br>
- $C$: 오분류 허용 정도  
  - $C\uparrow$ → 오분류 최소화, 과적합 위험↑  
  - $C\downarrow$ → 오분류 허용, 과소적합 위험↑  

- $\sigma$: 커널의 범위 조절  
  - $\sigma$ 작으면 → 개별 포인트에 민감 (variance↑, 과적합 위험↑)  
  - $\sigma$ 크면 → 데이터 전체에 완만 (bias↑, 과소적합 위험↑) 
<br>
<span style="color = red"> $\sigma$와 $C$ 같은 하이퍼파라미터 선택이 중요. </span>

<br>

    개인 공부 기록용 블로그입니다.
    오류나 틀린 부분이 있을 경우 언제든지 글이나 메일로 지적해주시면 감사하겠습니다! ☺

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}